# è®­ç»ƒè„šæœ¬ä½¿ç”¨è¯´æ˜

æœ¬ç›®å½•åŒ…å«ç”¨äºæ³•å¾‹åˆ¤å†³é¢„æµ‹ä»»åŠ¡çš„ Rejection Sampling + SFT è®­ç»ƒæµç¨‹ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
train/
â”œâ”€â”€ rejection_sampling.py    # æ‹’ç»é‡‡æ ·è„šæœ¬
â”œâ”€â”€ sft_train.py             # QLoRA SFTè®­ç»ƒè„šæœ¬
â””â”€â”€ README.md                # æœ¬æ–‡ä»¶
```

## ğŸš€ è®­ç»ƒæµç¨‹

### ç¬¬ä¸€æ­¥ï¼šæ‹’ç»é‡‡æ · (Rejection Sampling)

å¯¹è®­ç»ƒé›†ä¸­çš„æ¯ä¸ªæ ·æœ¬å¹¶è¡Œç”Ÿæˆ8æ¡å›å¤ï¼Œç­›é€‰å‡ºç½ªåå’Œåˆ‘æœŸå‡æ­£ç¡®çš„å›å¤ä½œä¸ºSFTè®­ç»ƒæ•°æ®ã€‚

#### ä½¿ç”¨æ–¹æ³•

```bash
python train/rejection_sampling.py \
    --model_path Qwen/Qwen2.5-0.5B-Instruct \
    --training_data LawShift/training_set.json \
    --articles LawShift/[æŸä¸ªç›®å½•]/articles_original.json \
    --output train/sampled_data.json \
    --num_samples 8 \
    --temperature 0.8 \
    --top_p 0.95
```

#### å‚æ•°è¯´æ˜

- `--model_path`: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨ Qwen3-0.6Bï¼‰
- `--training_data`: è®­ç»ƒæ•°æ®è·¯å¾„ï¼Œé»˜è®¤ä¸º `LawShift/training_set.json`
- `--articles`: æ³•æ¡æ•°æ®è·¯å¾„ï¼ˆéœ€è¦æŒ‡å®šå…·ä½“çš„articles_original.jsonæ–‡ä»¶ï¼‰
- `--output`: é‡‡æ ·ç»“æœè¾“å‡ºè·¯å¾„ï¼Œé»˜è®¤ä¸º `train/sampled_data.json`
- `--num_samples`: æ¯ä¸ªæ ·æœ¬é‡‡æ ·çš„å›å¤æ•°é‡ï¼Œé»˜è®¤ä¸º 8
- `--temperature`: é‡‡æ ·æ¸©åº¦ï¼Œé»˜è®¤ä¸º 0.8
- `--top_p`: nucleus sampling å‚æ•°ï¼Œé»˜è®¤ä¸º 0.95
- `--device`: è®¾å¤‡ç±»å‹ï¼Œé»˜è®¤ä¸º "auto"
- `--max_new_tokens`: æœ€å¤§ç”Ÿæˆ token æ•°ï¼Œé»˜è®¤ä¸º 2048

#### è¾“å‡ºæ–‡ä»¶

1. **sampled_data.json**: é‡‡æ ·å¾—åˆ°çš„è®­ç»ƒæ•°æ®ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
   - `fact`: æ¡ˆä»¶äº‹å®
   - `relevant_articles`: ç›¸å…³æ³•æ¡IDåˆ—è¡¨
   - `charge`: çœŸå®ç½ªå
   - `prison_time`: çœŸå®åˆ‘æœŸ
   - `response`: æ¨¡å‹ç”Ÿæˆçš„æ­£ç¡®å›å¤
   - `num_correct_for_this_sample`: è¯¥é—®é¢˜çš„æ­£ç¡®å›å¤æ€»æ•°ï¼ˆç”¨äºè®¡ç®—æƒé‡ï¼‰

2. **sampling_stats_*.json**: é‡‡æ ·ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
   - æ€»æ ·æœ¬æ•°ã€æ€»ç”Ÿæˆæ•°ã€æ€»æ¥å—æ•°
   - æ¥å—ç‡
   - æ— æ­£ç¡®å›å¤çš„æ ·æœ¬æ•°
   - å¹³å‡æ¯ä¸ªæ ·æœ¬çš„æ­£ç¡®å›å¤æ•°

### ç¬¬äºŒæ­¥ï¼šQLoRA SFTè®­ç»ƒ

ä½¿ç”¨æ‹’ç»é‡‡æ ·å¾—åˆ°çš„æ•°æ®è¿›è¡Œç›‘ç£å¾®è°ƒï¼Œé‡‡ç”¨QLoRAæ–¹æ³•é™ä½æ˜¾å­˜å ç”¨ã€‚

#### ç‰¹æ®Šè®¾è®¡ï¼šæ¢¯åº¦æƒé‡è°ƒæ•´

ä¸ºä¿è¯è®­ç»ƒçš„æ— åæ€§ï¼Œå¯¹äºåŒä¸€ä¸ªé—®é¢˜æœ‰ n ä¸ªæ­£ç¡®å›å¤çš„æƒ…å†µï¼Œæ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦ä¼šä¹˜ä»¥æƒé‡ `1/n`ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿ï¼š
- æ¯ä¸ªé—®é¢˜å¯¹æ¨¡å‹çš„å½±å“åŠ›ç›¸åŒ
- é¿å…æŸäº›é—®é¢˜å› ä¸ºæœ‰æ›´å¤šæ­£ç¡®å›å¤è€Œä¸»å¯¼è®­ç»ƒè¿‡ç¨‹

#### ä½¿ç”¨æ–¹æ³•

```bash
python train/sft_train.py \
    --model_path Qwen/Qwen2.5-0.5B-Instruct \
    --sampled_data train/sampled_data.json \
    --articles_path LawShift/[æŸä¸ªç›®å½•]/articles_original.json \
    --output_dir train/checkpoints \
    --num_train_epochs 3 \
    --per_device_train_batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 2e-4
```

#### å‚æ•°è¯´æ˜

**æ¨¡å‹ç›¸å…³**
- `--model_path`: åŸºç¡€æ¨¡å‹è·¯å¾„
- `--output_dir`: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º `train/checkpoints`

**æ•°æ®ç›¸å…³**
- `--sampled_data`: æ‹’ç»é‡‡æ ·å¾—åˆ°çš„æ•°æ®è·¯å¾„ï¼Œé»˜è®¤ä¸º `train/sampled_data.json`
- `--articles_path`: æ³•æ¡æ•°æ®è·¯å¾„
- `--max_length`: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œé»˜è®¤ä¸º 4096

**LoRAç›¸å…³**
- `--lora_r`: LoRA rankï¼Œé»˜è®¤ä¸º 64
- `--lora_alpha`: LoRA alphaï¼Œé»˜è®¤ä¸º 16
- `--lora_dropout`: LoRA dropoutï¼Œé»˜è®¤ä¸º 0.05

**è®­ç»ƒç›¸å…³**
- `--num_train_epochs`: è®­ç»ƒè½®æ•°ï¼Œé»˜è®¤ä¸º 3
- `--per_device_train_batch_size`: æ¯ä¸ªè®¾å¤‡çš„æ‰¹å¤§å°ï¼Œé»˜è®¤ä¸º 2
- `--gradient_accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œé»˜è®¤ä¸º 8ï¼ˆå®é™…æ‰¹å¤§å° = 2 Ã— 8 = 16ï¼‰
- `--learning_rate`: å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º 2e-4
- `--warmup_steps`: é¢„çƒ­æ­¥æ•°ï¼Œé»˜è®¤ä¸º 100
- `--logging_steps`: æ—¥å¿—è®°å½•æ­¥æ•°ï¼Œé»˜è®¤ä¸º 10
- `--save_steps`: ä¿å­˜æ­¥æ•°ï¼Œé»˜è®¤ä¸º 100
- `--weight_decay`: æƒé‡è¡°å‡ï¼Œé»˜è®¤ä¸º 0.01

**é‡åŒ–ç›¸å…³**
- `--no_4bit`: ä¸ä½¿ç”¨4bité‡åŒ–ï¼ˆé»˜è®¤ä½¿ç”¨4bité‡åŒ–ï¼‰

#### è¾“å‡ºæ–‡ä»¶

è®­ç»ƒå®Œæˆåï¼Œä¼šåœ¨è¾“å‡ºç›®å½•ä¸‹ç”Ÿæˆï¼š
- `final_model/`: æœ€ç»ˆè®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆLoRAæƒé‡ï¼‰
- `checkpoint-*/`: ä¸­é—´æ£€æŸ¥ç‚¹
- `logs/`: TensorBoardæ—¥å¿—æ–‡ä»¶

## ğŸ’¡ å®Œæ•´è®­ç»ƒç¤ºä¾‹

```bash
# 1. æ‹’ç»é‡‡æ ·ï¼ˆä½¿ç”¨Qwen3-0.6BåŸºç¡€æ¨¡å‹ï¼‰
python train/rejection_sampling.py \
    --model_path Qwen/Qwen2.5-0.5B-Instruct \
    --training_data LawShift/training_set.json \
    --articles LawShift/term_up/articles_original.json \
    --output train/sampled_data.json \
    --num_samples 8

# 2. QLoRA SFTè®­ç»ƒ
python train/sft_train.py \
    --model_path Qwen/Qwen2.5-0.5B-Instruct \
    --sampled_data train/sampled_data.json \
    --articles_path LawShift/term_up/articles_original.json \
    --output_dir train/checkpoints \
    --num_train_epochs 3

# 3. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
python eval/evaluate.py \
    --model_path train/checkpoints/sft_20250115_120000/final_model \
    --base_model Qwen/Qwen2.5-0.5B-Instruct \
    --test_dir LawShift/term_up \
    --output_dir results/finetuned_model
```

## ğŸ“Š è®­ç»ƒç›‘æ§

ä½¿ç”¨TensorBoardæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ï¼š

```bash
tensorboard --logdir train/checkpoints/sft_*/logs
```

## âš™ï¸ æ˜¾å­˜éœ€æ±‚

- **æ‹’ç»é‡‡æ ·**: éœ€è¦çº¦ 8-12 GB æ˜¾å­˜ï¼ˆå–å†³äºæ¨¡å‹å¤§å°ï¼‰
- **QLoRAè®­ç»ƒ**: éœ€è¦çº¦ 12-16 GB æ˜¾å­˜ï¼ˆä½¿ç”¨4bité‡åŒ–ï¼‰

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š
1. å‡å° `--per_device_train_batch_size`
2. å¢å¤§ `--gradient_accumulation_steps`ï¼ˆä¿æŒå®é™…æ‰¹å¤§å°ä¸å˜ï¼‰
3. å‡å° `--max_length`

## ğŸ¯ å…³é”®ç‰¹æ€§

1. **æ‹’ç»é‡‡æ ·**: é€šè¿‡å¹¶è¡Œç”Ÿæˆå¤šæ¡å›å¤å¹¶ç­›é€‰æ­£ç¡®çš„ï¼Œæé«˜è®­ç»ƒæ•°æ®è´¨é‡
2. **æ¢¯åº¦æƒé‡è°ƒæ•´**: é€šè¿‡ 1/n æƒé‡ç¡®ä¿è®­ç»ƒæ— åæ€§
3. **QLoRAè®­ç»ƒ**: ä½¿ç”¨4bité‡åŒ–+LoRAï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨
4. **çµæ´»é…ç½®**: æ”¯æŒä¸°å¯Œçš„è®­ç»ƒå‚æ•°é…ç½®

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **è®­ç»ƒæ•°æ®**: `training_set.json` è¾ƒå¤§ï¼Œæ‹’ç»é‡‡æ ·å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
2. **æ³•æ¡æ•°æ®**: ä¸åŒçš„æ•°æ®é›†å­ç›®å½•æœ‰ä¸åŒçš„æ³•æ¡æ–‡ä»¶ï¼Œéœ€è¦æŒ‡å®šæ­£ç¡®çš„è·¯å¾„
3. **æ¨¡å‹è·¯å¾„**: ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
4. **è¾“å‡ºæ ¼å¼**: è®­ç»ƒç‰ˆpromptè¦æ±‚æ¨¡å‹è¾“å‡º "{ç½ªå} | {åˆ‘æœŸ}" æˆ– "ä¸è¿è§„"
5. **æ­»åˆ‘/æ— æœŸå¾’åˆ‘**: åˆ‘æœŸä¸ºæ­»åˆ‘æˆ–æ— æœŸå¾’åˆ‘æ—¶ï¼Œåº”è¾“å‡º "XT" è€Œéæ•°å­—

## ğŸ”§ ä¾èµ–å®‰è£…

```bash
pip install torch transformers accelerate peft bitsandbytes datasets tqdm tensorboard
```

## ğŸ“ é—®é¢˜åé¦ˆ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š
1. æ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®
2. æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
3. æ˜¾å­˜æ˜¯å¦å……è¶³
4. Pythonç¯å¢ƒæ˜¯å¦æ­£ç¡®å®‰è£…æ‰€æœ‰ä¾èµ–
