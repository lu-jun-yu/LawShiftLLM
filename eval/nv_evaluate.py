"""
NVï¼ˆæ— ç½ªï¼‰ç±»å‹æ•°æ®é›†æµ‹è¯„è„šæœ¬
ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ NV_construct æ•°æ®é›†ä¸Šçš„æ€§èƒ½
"""

import json
import re
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
import yaml

try:
    from ..prompt_template import SYSTEM_PROMPT, format_user_prompt, format_articles
except ImportError:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from prompt_template import SYSTEM_PROMPT, format_user_prompt, format_articles


class NVEvaluator:

    def __init__(self, model_path: str, device: str = "auto", use_flash_attn: bool = False,
                 temperature: float = 0.7, top_p: float = 0.9, max_tokens: int = 1024,
                 num_samples: int = 1, max_prompt_length: int = 0):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹ (auto/cuda/cpu)
            use_flash_attn: æ˜¯å¦ä½¿ç”¨ Flash Attention 2
            temperature: æ¸©åº¦å‚æ•°
            top_p: top-pé‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            num_samples: é‡‡æ ·æ¬¡æ•°ï¼ˆ>1æ—¶å¯¹åŒä¸€promptå¤šæ¬¡é‡‡æ ·ï¼ŒæŒ‡æ ‡å–å¹³å‡å€¼ï¼‰
            max_prompt_length: æœ€å¤§prompté•¿åº¦ï¼ˆtokenæ•°ï¼‰ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.model_path = model_path
        self.device = device
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.num_samples = num_samples
        self.max_prompt_length = max_prompt_length

        # åŠ è½½æ³•æ¡
        self.articles = {}
        articles_path = Path(__file__).parent.parent / "articles.json"
        if articles_path.exists():
            with open(articles_path, 'r', encoding='utf-8') as f:
                self.articles = json.load(f)
            print(f"å·²åŠ è½½ {len(self.articles)} æ¡æ³•æ¡")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° articles.json æ–‡ä»¶: {articles_path}")

        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left"
        )

        # è®¾ç½® pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹ä¸­...")
        model_kwargs = {
            "trust_remote_code": True,
            "device_map": device,
        }

        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦
        if torch.cuda.is_available():
            if torch.cuda.is_bf16_supported():
                model_kwargs["dtype"] = torch.bfloat16
                print("ä½¿ç”¨ BF16 ç²¾åº¦")
            else:
                model_kwargs["dtype"] = torch.float16
                print("ä½¿ç”¨ FP16 ç²¾åº¦")
        else:
            model_kwargs["dtype"] = torch.float32

        # å°è¯•ä½¿ç”¨ Flash Attention 2
        if use_flash_attn:
            try:
                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("ä½¿ç”¨ Flash Attention 2 åŠ é€Ÿ")
            except Exception:
                print("Flash Attention 2 ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›")

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **model_kwargs
        )

        self.model.eval()

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    def load_nv_data(self, file_path: str) -> List[Dict]:
        """
        åŠ è½½ NV æ•°æ®æ–‡ä»¶

        Args:
            file_path: JSON æ–‡ä»¶è·¯å¾„

        Returns:
            æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« fact, relevant_articles, label
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data

    def generate_predictions_batch(self, prompts: List[str]) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆé¢„æµ‹

        Args:
            prompts: æç¤ºè¯åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        # æ‰¹é‡ç¼–ç ï¼Œä½¿ç”¨padding
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # æ‰¹é‡ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.max_tokens,
                temperature=self.temperature,
                top_p=self.top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # æ‰¹é‡è§£ç 
        responses = []
        input_lengths = inputs['input_ids'].shape[1]
        for output in outputs:
            # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response = self.tokenizer.decode(
                output[input_lengths:],
                skip_special_tokens=True
            )
            responses.append(response)

        return responses

    def parse_prediction(self, response: str) -> Tuple[str, str]:
        """
        è§£ææ¨¡å‹è¾“å‡º

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            (è¿è§„åˆ¤æ–­, åˆ‘æœŸ)
            - è¿è§„åˆ¤æ–­: "V" æˆ– "NV"
            - åˆ‘æœŸ: æ•°å­—å­—ç¬¦ä¸²ã€"XT"ã€Noneï¼ˆå¦‚æœåªæœ‰V/NVï¼‰
        """
        # æ¨¡å¼1: <answer> NV </answer>
        if re.search(r'<answer>\s*NV\s*</answer>', response, re.IGNORECASE):
            return "NV", None

        # æ¨¡å¼2: <answer> V | XT </answer> (æ­»åˆ‘/æ— æœŸ)
        xt_pattern = r'<answer>\s*V\s*\|\s*XT\s*</answer>'
        if re.search(xt_pattern, response, re.IGNORECASE):
            return "V", "XT"

        # æ¨¡å¼3: <answer> V | {æ•°å­—} </answer> (æ ‡å‡†è¿è§„+åˆ‘æœŸ)
        v_prison_pattern = r'<answer>\s*V\s*\|\s*(\d+)\s*</answer>'
        match = re.search(v_prison_pattern, response, re.IGNORECASE)
        if match:
            prison_time = match.group(1).strip()
            return "V", prison_time

        # æ¨¡å¼4: <answer> V </answer> (åªæœ‰è¿è§„åˆ¤æ–­)
        if re.search(r'<answer>\s*V\s*</answer>', response, re.IGNORECASE):
            return "V", None

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›æœªè¯†åˆ«
        return "æœªè¯†åˆ«", None

    def check_prediction_success(self, pred_violation: str, label: str) -> bool:
        """
        åˆ¤æ–­é¢„æµ‹æ˜¯å¦æˆåŠŸ

        Args:
            pred_violation: é¢„æµ‹çš„è¿è§„åˆ¤æ–­ ("V" æˆ– "NV")
            label: æ ‡ç­¾ï¼ˆå¯¹äº NV æ•°æ®é›†ï¼Œæ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯ "NV"ï¼‰

        Returns:
            æ˜¯å¦é¢„æµ‹æˆåŠŸ
        """
        # NV æ•°æ®é›†ï¼šé¢„æµ‹ç»“æœåº”è¯¥æ˜¯ NV
        return pred_violation == label

    def evaluate_file(self, file_path: str, batch_size: int = 8) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ª NV æ•°æ®æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        file_name = Path(file_path).stem
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è¯„ä¼°: {file_name}")
        print(f"{'='*60}")

        data = self.load_nv_data(file_path)
        print(f"åŸå§‹æ•°æ®æ¡æ•°: {len(data)}")

        # é¢„å¤„ç†ï¼šæ„å»ºæ‰€æœ‰ prompt å¹¶è¿‡æ»¤è¶…é•¿çš„
        processed_data = []
        skipped_count = 0

        for item in data:
            fact = item["fact"]
            article_ids = item["relevant_articles"]

            # è·å–æ³•æ¡å†…å®¹
            articles_text = format_articles(self.articles, article_ids)
            user_prompt = format_user_prompt(fact, articles_text)

            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ]

            if hasattr(self.tokenizer, 'apply_chat_template'):
                prompt = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                ) + " <think>\n"
            else:
                prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_prompt}\n\nAssistant: <think>\n"

            # æ£€æŸ¥ prompt é•¿åº¦
            if self.max_prompt_length > 0:
                prompt_tokens = self.tokenizer.encode(prompt, add_special_tokens=False)
                if len(prompt_tokens) > self.max_prompt_length:
                    skipped_count += 1
                    continue

            processed_data.append({
                "item": item,
                "prompt": prompt
            })

        if self.max_prompt_length > 0:
            print(f"è¿‡æ»¤è¶…é•¿promptåæ•°æ®æ¡æ•°: {len(processed_data)} (è·³è¿‡ {skipped_count} æ¡ï¼Œè¶…è¿‡ {self.max_prompt_length} tokens)")

        results = {
            "file": file_name,
            "correct": 0,
            "total": 0,
            "skipped": skipped_count,
            "predictions": []
        }

        num_samples = self.num_samples
        if num_samples > 1:
            print(f"  ä½¿ç”¨ {num_samples} æ¬¡é‡‡æ ·")
            results["num_samples"] = num_samples

        for i in tqdm(range(0, len(processed_data), batch_size), desc=file_name):
            batch_data = processed_data[i:i + batch_size]
            batch = [d["item"] for d in batch_data]
            prompts = [d["prompt"] for d in batch_data]

            try:
                # å¤šæ¬¡é‡‡æ ·
                all_sample_results = []

                for sample_idx in range(num_samples):
                    responses = self.generate_predictions_batch(prompts)
                    sample_results = []
                    for response in responses:
                        pred_violation, pred_prison = self.parse_prediction(response)
                        sample_results.append((pred_violation, pred_prison, response))
                    all_sample_results.append(sample_results)

                # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„å¤šæ¬¡é‡‡æ ·ç»“æœ
                for item_idx, (item, prompt) in enumerate(zip(batch, prompts)):
                    fact = item["fact"]
                    article_ids = item["relevant_articles"]
                    label = item["label"]
                    relevant_articles_texts = [self.articles.get(str(aid), f"Article {aid} not found") for aid in article_ids]

                    # æ”¶é›†è¯¥æ ·æœ¬åœ¨æ‰€æœ‰é‡‡æ ·ä¸­çš„ç»“æœ
                    pred_violations = []
                    pred_prisons = []
                    full_responses = []
                    correct_count = 0

                    for sample_idx in range(num_samples):
                        pred_violation, pred_prison, response = all_sample_results[sample_idx][item_idx]
                        pred_violations.append(pred_violation)
                        pred_prisons.append(pred_prison)
                        full_responses.append(response)

                        is_correct = self.check_prediction_success(pred_violation, label)
                        if is_correct:
                            correct_count += 1

                    # è®¡ç®—è¯¥æ ·æœ¬çš„å¹³å‡å‡†ç¡®ç‡
                    avg_correct = correct_count / num_samples
                    results["correct"] += avg_correct

                    prediction_record = {
                        "sample_id": results["total"],
                        "pred_violation": pred_violations if num_samples > 1 else pred_violations[0],
                        "pred_prison": pred_prisons if num_samples > 1 else pred_prisons[0],
                        "label": label,
                        "is_correct": correct_count == num_samples if num_samples == 1 else None,
                        "fact": fact,
                        "relevant_articles": relevant_articles_texts,
                        "full_prompt": prompt,
                        "full_response": full_responses if num_samples > 1 else full_responses[0]
                    }

                    # å¦‚æœå¤šæ¬¡é‡‡æ ·ï¼Œæ·»åŠ é¢å¤–ä¿¡æ¯
                    if num_samples > 1:
                        prediction_record["num_samples"] = num_samples
                        prediction_record["correct_count"] = correct_count
                        prediction_record["avg_correct"] = avg_correct

                    results["predictions"].append(prediction_record)
                    results["total"] += 1

            except Exception as e:
                print(f"\næ‰¹é‡é¢„æµ‹å‡ºé”™: {e}")
                for item, prompt in zip(batch, prompts):
                    article_ids = item["relevant_articles"]
                    relevant_articles_texts = [self.articles.get(str(aid), f"Article {aid} not found") for aid in article_ids]

                    results["predictions"].append({
                        "sample_id": results["total"],
                        "error": str(e),
                        "fact": item["fact"],
                        "label": item["label"],
                        "relevant_articles": relevant_articles_texts,
                        "full_prompt": prompt
                    })
                    results["total"] += 1

        # è®¡ç®—å‡†ç¡®ç‡
        if results["total"] > 0:
            results["accuracy"] = results["correct"] / results["total"]

        # æ‰“å°ç»“æœ
        self.print_results(results)

        return results

    def print_results(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°ç»“æœ: {results['file']}")
        print(f"{'='*60}")

        print(f"  æ€»æ•°: {results['total']}")
        print(f"  æ­£ç¡®æ•°: {results['correct']}")
        print(f"  å‡†ç¡®ç‡: {results.get('accuracy', 0):.2%}")

        print("\nã€è¯„ä¼°æ ‡å‡†ã€‘")
        print(f"  é¢„æµ‹ç»“æœä¸º NV â†’ æˆåŠŸ")

    def evaluate_all(self, nv_data_root: str = "./NV_construct/output", batch_size: int = 8,
                     output_dir: str = "./results", resume: bool = False) -> Tuple[List[Dict[str, Any]], str]:
        """
        è¯„ä¼°æ‰€æœ‰ NV æ•°æ®æ–‡ä»¶

        Args:
            nv_data_root: NV æ•°æ®é›†æ ¹ç›®å½•
            batch_size: æ‰¹é‡å¤§å°
            output_dir: è¾“å‡ºç›®å½•
            resume: æ˜¯å¦ä»å·²æœ‰ç»“æœæ¢å¤

        Returns:
            (æ‰€æœ‰è¯„ä¼°ç»“æœåˆ—è¡¨, ç»“æœä¿å­˜ç›®å½•)
        """
        nv_data_path = Path(nv_data_root)
        all_results = []

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = Path(self.model_path).name

        # å¦‚æœæ˜¯resumeæ¨¡å¼ï¼Œä½¿ç”¨å·²æœ‰çš„output_dirï¼›å¦åˆ™åˆ›å»ºæ–°çš„å¸¦æ—¶é—´æˆ³çš„ç›®å½•
        if resume:
            results_dir = Path(output_dir)
            if not results_dir.exists():
                print(f"è­¦å‘Š: æŒ‡å®šçš„output_dirä¸å­˜åœ¨: {results_dir}")
                print("å°†åˆ›å»ºæ–°çš„è¯„ä¼°ç›®å½•...")
                results_dir = Path(output_dir) / f"nv_{model_name}_{timestamp}"
                results_dir.mkdir(parents=True, exist_ok=True)
                resume = False
            else:
                print(f"\nä»å·²æœ‰ç›®å½•æ¢å¤è¯„ä¼°: {results_dir}")
                # åŠ è½½å·²æœ‰çš„è¯„ä¼°ç»“æœ
                for result_file in results_dir.glob("*_results.json"):
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            result = json.load(f)
                            all_results.append(result)
                        print(f"å·²åŠ è½½: {result_file.name}")
                    except Exception as e:
                        print(f"åŠ è½½ {result_file.name} æ—¶å‡ºé”™: {e}")
                print(f"å·²åŠ è½½ {len(all_results)} ä¸ªå·²å®Œæˆçš„è¯„ä¼°ç»“æœ")
        else:
            results_dir = Path(output_dir) / f"nv_{model_name}_{timestamp}"
            results_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nç»“æœå°†ä¿å­˜è‡³: {results_dir}")

        # è·å–å·²å®Œæˆçš„æ–‡ä»¶åç§°
        completed_files = {result["file"] for result in all_results}

        # éå†æ‰€æœ‰ JSON æ–‡ä»¶
        for json_file in sorted(nv_data_path.glob("*.json")):
            file_name = json_file.stem

            # å¦‚æœæ˜¯resumeæ¨¡å¼ä¸”è¯¥æ–‡ä»¶å·²å®Œæˆï¼Œåˆ™è·³è¿‡
            if resume and file_name in completed_files:
                print(f"\nè·³è¿‡å·²å®Œæˆçš„æ–‡ä»¶: {file_name}")
                continue

            try:
                results = self.evaluate_file(str(json_file), batch_size=batch_size)
                all_results.append(results)

                print(f"\nğŸ’¾ ä¿å­˜å½“å‰ç»“æœ ({len(all_results)} ä¸ªæ–‡ä»¶å·²å®Œæˆ)...")
                self.save_results(all_results, str(results_dir))

            except Exception as e:
                print(f"\nè¯„ä¼° {json_file.name} æ—¶å‡ºé”™: {e}")
                continue

        # æ‰“å°æ€»ä½“ç»“æœ
        self.print_summary(all_results)

        return all_results, str(results_dir)

    def print_summary(self, all_results: List[Dict[str, Any]]):
        """æ‰“å°æ€»ä½“è¯„ä¼°ç»“æœæ‘˜è¦"""
        print(f"\n{'='*60}")
        print("æ€»ä½“è¯„ä¼°ç»“æœæ‘˜è¦")
        print(f"{'='*60}")

        total_correct = 0
        total_samples = 0

        for result in all_results:
            file_name = result["file"]
            accuracy = result.get("accuracy", 0)
            correct = result["correct"]
            total = result["total"]

            print(f"  {file_name}: {accuracy:.2%} ({correct}/{total})")

            total_correct += correct
            total_samples += total

        if total_samples > 0:
            overall_accuracy = total_correct / total_samples
            print(f"\n  æ€»ä½“å‡†ç¡®ç‡: {overall_accuracy:.2%} ({total_correct}/{total_samples})")

    def save_results(self, all_results: List[Dict[str, Any]], output_dir: str = "./results"):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            all_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for result in all_results:
            file_name = result["file"]
            file_result = {
                "file": file_name,
                "correct": result["correct"],
                "total": result["total"],
                "accuracy": result.get("accuracy"),
                "predictions": result["predictions"]
            }

            result_file = output_path / f"{file_name}_results.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(file_result, f, ensure_ascii=False, indent=2)
            print(f"å·²ä¿å­˜: {result_file}")

        # ä¿å­˜æ‘˜è¦
        summary = {
            "total_files": len(all_results),
            "results": []
        }

        total_correct = 0
        total_samples = 0

        for result in all_results:
            summary["results"].append({
                "file": result["file"],
                "correct": result["correct"],
                "total": result["total"],
                "accuracy": result.get("accuracy")
            })
            total_correct += result["correct"]
            total_samples += result["total"]

        if total_samples > 0:
            summary["overall_accuracy"] = total_correct / total_samples
            summary["overall_correct"] = total_correct
            summary["overall_total"] = total_samples

        summary_file = output_path / "summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"å·²ä¿å­˜æ‘˜è¦: {summary_file}")


def load_config(config_path: str) -> dict:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="NVæ•°æ®é›†è¯„ä¼°è„šæœ¬")
    parser.add_argument(
        "--config",
        type=str,
        default="config/nv_evaluate.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆYAMLæ ¼å¼ï¼‰"
    )
    parser.add_argument(
        "--model_path",
        type=str,
        default=None,
        help="æ¨¡å‹è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "--nv_data_root",
        type=str,
        default=None,
        help="NVæ•°æ®é›†è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=None,
        help="æ‰¹é‡å¤§å°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰"
    )
    args = parser.parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶
    config_path = Path(args.config)
    if config_path.exists():
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°: {config_path}")
        config = load_config(str(config_path))
    else:
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ ({config_path})ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        config = {
            "model": {
                "model_path": "./models/Qwen2.5-7B-Instruct",
                "device": "auto",
                "use_flash_attn": False
            },
            "data": {
                "nv_data_root": "./NV_construct/output",
                "output_dir": "./results"
            },
            "inference": {
                "batch_size": 8,
                "num_samples": 1,
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": 1024,
                "max_prompt_length": 0
            },
            "evaluation": {
                "resume": False
            }
        }

    # ä»é…ç½®æ–‡ä»¶æå–å‚æ•°ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
    model_path = args.model_path or config["model"]["model_path"]
    device = config["model"]["device"]
    use_flash_attn = config["model"]["use_flash_attn"]

    nv_data_root = args.nv_data_root or config["data"]["nv_data_root"]
    output_dir = args.output_dir or config["data"]["output_dir"]

    batch_size = args.batch_size or config["inference"]["batch_size"]
    num_samples = config["inference"]["num_samples"]
    temperature = config["inference"]["temperature"]
    top_p = config["inference"]["top_p"]
    max_tokens = config["inference"]["max_tokens"]
    max_prompt_length = config["inference"].get("max_prompt_length", 0)

    resume = config["evaluation"]["resume"]

    print("="*80)
    print("NV æ•°æ®é›†è¯„ä¼°")
    print("="*80)
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æ•°æ®é›†è·¯å¾„: {nv_data_root}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    print(f"è®¾å¤‡: {device}")
    print(f"Flash Attention: {use_flash_attn}")
    print(f"é‡‡æ ·æ¬¡æ•°: {num_samples}")
    print(f"æ¸©åº¦: {temperature}")
    print(f"Top-p: {top_p}")
    print(f"æœ€å¤§tokenæ•°: {max_tokens}")
    print(f"æœ€å¤§prompté•¿åº¦: {max_prompt_length if max_prompt_length > 0 else 'ä¸é™åˆ¶'}")
    print(f"æ¢å¤æ¨¡å¼: {resume}")
    print("="*80)

    evaluator = NVEvaluator(
        model_path,
        device=device,
        use_flash_attn=use_flash_attn,
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        num_samples=num_samples,
        max_prompt_length=max_prompt_length
    )

    all_results, results_dir = evaluator.evaluate_all(
        nv_data_root,
        batch_size=batch_size,
        output_dir=output_dir,
        resume=resume
    )

    print("\nè¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
