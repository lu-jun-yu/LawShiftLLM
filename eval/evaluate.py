"""
æ³•å¾‹åˆ¤å†³é¢„æµ‹ï¼ˆLJPï¼‰æµ‹è¯„è„šæœ¬
ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨LawShiftæ•°æ®é›†ä¸Šçš„æ€§èƒ½
"""

import json
import re
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

try:
    from ..prompt_template import SYSTEM_PROMPT, format_user_prompt, format_user_prompt_poisoned, format_articles
except ImportError:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from prompt_template import SYSTEM_PROMPT, format_user_prompt, format_user_prompt_poisoned, format_articles


class LawShiftEvaluator:

    def __init__(self, model_path: str, device: str = "auto", use_flash_attn: bool = False):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹ (auto/cuda/cpu)
            use_flash_attn: æ˜¯å¦ä½¿ç”¨ Flash Attention 2
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.model_path = model_path
        self.device = device

        # åŠ è½½ label æ˜ å°„
        self.label_mapping = {}
        label_path = Path(__file__).parent.parent / "label.json"

        if label_path.exists():
            with open(label_path, 'r', encoding='utf-8') as f:
                labels = json.load(f)
                for item in labels:
                    self.label_mapping[item['name']] = item['label']
            print(f"å·²åŠ è½½ {len(self.label_mapping)} ä¸ªæ ‡ç­¾æ˜ å°„")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° label.json æ–‡ä»¶: {label_path}")

        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left"
        )

        # è®¾ç½® pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹ä¸­...")
        model_kwargs = {
            "trust_remote_code": True,
            "device_map": device,
        }

        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦
        if torch.cuda.is_available():
            if torch.cuda.is_bf16_supported():
                model_kwargs["dtype"] = torch.bfloat16
                print("ä½¿ç”¨ BF16 ç²¾åº¦")
            else:
                model_kwargs["dtype"] = torch.float16
                print("ä½¿ç”¨ FP16 ç²¾åº¦")
        else:
            model_kwargs["dtype"] = torch.float32

        # å°è¯•ä½¿ç”¨ Flash Attention 2
        if use_flash_attn:
            try:
                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("ä½¿ç”¨ Flash Attention 2 åŠ é€Ÿ")
            except Exception:
                print("Flash Attention 2 ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›")

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **model_kwargs
        )

        self.model.eval()

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    def load_data(self, folder_path: str) -> Tuple[Dict, Dict, List[Dict], List[Dict]]:
        """
        åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            (articles_original, articles_poisoned, data_original, data_poisoned)
        """
        folder = Path(folder_path)

        # åŠ è½½æ³•æ¡
        with open(folder / "articles_original.json", 'r', encoding='utf-8') as f:
            articles_original = json.load(f)

        with open(folder / "articles_poisoned.json", 'r', encoding='utf-8') as f:
            articles_poisoned = json.load(f)

        # åŠ è½½æµ‹è¯•æ•°æ®
        with open(folder / "original.json", 'r', encoding='utf-8') as f:
            data_original = json.load(f)

        with open(folder / "poisoned.json", 'r', encoding='utf-8') as f:
            data_poisoned = json.load(f)

        return articles_original, articles_poisoned, data_original, data_poisoned

    def generate_prediction(self, fact: str, articles_text: str) -> str:
        """
        ç”Ÿæˆé¢„æµ‹

        Args:
            fact: æ¡ˆä»¶äº‹å®
            articles_text: ç›¸å…³æ³•æ¡æ–‡æœ¬

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        """
        # æ„å»ºæç¤ºè¯
        user_prompt = format_user_prompt(fact, articles_text)

        # æ„å»ºå¯¹è¯æ¶ˆæ¯
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]

        # ä½¿ç”¨chat template
        if hasattr(self.tokenizer, 'apply_chat_template'):
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # å¦‚æœæ²¡æœ‰chat templateï¼Œæ‰‹åŠ¨æ„å»º
            prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_prompt}\n\nAssistant:"

        # ç¼–ç å¹¶ç§»åˆ°GPU
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return response

    def generate_predictions_batch(self, prompts: List[str]) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆé¢„æµ‹

        Args:
            prompts: æç¤ºè¯åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        # æ‰¹é‡ç¼–ç ï¼Œä½¿ç”¨padding
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # æ‰¹é‡ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # æ‰¹é‡è§£ç 
        responses = []
        input_lengths = inputs['input_ids'].shape[1]
        for output in outputs:
            # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response = self.tokenizer.decode(
                output[input_lengths:],
                skip_special_tokens=True
            )
            responses.append(response)

        return responses

    def parse_prediction(self, response: str) -> Tuple[str, str]:
        """
        è§£ææ¨¡å‹è¾“å‡º

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            (è¿è§„åˆ¤æ–­, åˆ‘æœŸ)
            - è¿è§„åˆ¤æ–­: "V" æˆ– "NV"
            - åˆ‘æœŸ: æ•°å­—å­—ç¬¦ä¸²ã€"XT"ã€Noneï¼ˆå¦‚æœåªæœ‰V/NVï¼‰
        """
        # æ¨¡å¼1: <answer> NV </answer>
        if re.search(r'<answer>\s*NV\s*</answer>', response, re.IGNORECASE):
            return "NV", None

        # æ¨¡å¼2: <answer> V | XT </answer> (æ­»åˆ‘/æ— æœŸ)
        xt_pattern = r'<answer>\s*V\s*\|\s*XT\s*</answer>'
        if re.search(xt_pattern, response, re.IGNORECASE):
            return "V", "XT"

        # æ¨¡å¼3: <answer> V | {æ•°å­—} </answer> (æ ‡å‡†è¿è§„+åˆ‘æœŸ)
        v_prison_pattern = r'<answer>\s*V\s*\|\s*(\d+)\s*</answer>'
        match = re.search(v_prison_pattern, response, re.IGNORECASE)
        if match:
            prison_time = match.group(1).strip()
            return "V", prison_time

        # æ¨¡å¼4: <answer> V </answer> (åªæœ‰è¿è§„åˆ¤æ–­)
        if re.search(r'<answer>\s*V\s*</answer>', response, re.IGNORECASE):
            return "V", None

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›æœªè¯†åˆ«
        return "æœªè¯†åˆ«", None

    def check_prediction_success(self, pred_violation: str, pred_prison: str,
                                  label_type: str, split_name: str) -> bool:
        """
        æ ¹æ®ä¸åŒçš„ label ç±»å‹åˆ¤æ–­é¢„æµ‹æ˜¯å¦æˆåŠŸ

        Args:
            pred_violation: é¢„æµ‹çš„è¿è§„åˆ¤æ–­ ("V" æˆ– "NV")
            pred_prison: é¢„æµ‹çš„åˆ‘æœŸ (æ•°å­—å­—ç¬¦ä¸²ã€"XT" æˆ– None)
            label_type: æ ‡ç­¾ç±»å‹ (V, NV, TU, TD, XT, NX)
            split_name: åˆ†æ”¯åç§° (Original, Poisoned)

        Returns:
            æ˜¯å¦é¢„æµ‹æˆåŠŸ
        """
        if split_name == "Original":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V" or label_type == "NV":
                return pred_violation == "V"

            elif label_type == "TU" or label_type == "TD":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 <= int(pred_prison) <= 120
                return False

            elif label_type == "XT":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 <= int(pred_prison) <= 120
                return False

            elif label_type == "NX":
                return pred_violation == "V" and pred_prison == "XT"

        elif split_name == "Poisoned":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V":
                # label=V: é¢„æµ‹ç»“æœä¸º Vï¼ˆä¸éœ€è¦è€ƒè™‘åˆ‘æœŸï¼‰
                return pred_violation == "V"

            elif label_type == "NV":
                # label=NV: é¢„æµ‹ç»“æœä¸º NV
                return pred_violation == "NV"

            elif label_type == "TU":
                # label=TU: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 120
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) >= 120
                return False

            elif label_type == "TD":
                # label=TD: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT < 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) <= 36
                return False

            elif label_type == "XT":
                # label=XT: é¢„æµ‹ç»“æœä¸º 'V | XT'
                return pred_violation == "V" and pred_prison == "XT"

            elif label_type == "NX":
                # label=NX: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) >= 36
                return False

        # æœªçŸ¥ label ç±»å‹
        return False

    def evaluate_dataset(self, folder_path: str, batch_size: int = 8, evaluate_type: str = "all") -> Dict[str, Any]:
        """
        è¯„ä¼°æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®é›†ï¼ˆä½¿ç”¨æ‰¹é‡æ¨ç†ï¼‰

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            batch_size: æ‰¹é‡å¤§å°
            evaluate_type: è¯„ä¼°ç±»å‹ (original/poisoned/all)

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        folder_name = Path(folder_path).name
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è¯„ä¼°: {folder_name}")
        print(f"{'='*60}")

        label_type = self.label_mapping.get(folder_name, None)
        if label_type:
            print(f"æ ‡ç­¾ç±»å‹: {label_type}")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ä»¶å¤¹ '{folder_name}' çš„æ ‡ç­¾ç±»å‹")

        articles_orig, articles_pois, data_orig, data_pois = self.load_data(folder_path)

        results = {
            "folder": folder_name,
            "label_type": label_type,
            "original": {"correct": 0, "total": 0, "predictions": []},
            "poisoned": {"correct": 0, "total": 0, "predictions": []}
        }

        if evaluate_type in ["original", "all"]:
            print(f"\nè¯„ä¼° original.json (å…±{len(data_orig)}æ¡)")
            self._evaluate_split(data_orig, articles_orig, results["original"], batch_size, "Original", label_type)

        if evaluate_type in ["poisoned", "all"]:
            print(f"\nè¯„ä¼° poisoned.json (å…±{len(data_pois)}æ¡)")
            self._evaluate_split(data_pois, articles_pois, results["poisoned"], batch_size, "Poisoned", label_type, articles_orig)

        # è®¡ç®—å‡†ç¡®ç‡
        if results["original"]["total"] > 0:
            results["original"]["accuracy"] = results["original"]["correct"] / results["original"]["total"]

        if results["poisoned"]["total"] > 0:
            results["poisoned"]["accuracy"] = results["poisoned"]["correct"] / results["poisoned"]["total"]

        # æ‰“å°ç»“æœ
        self.print_results(results)

        return results

    def _evaluate_split(self, data: List[Dict], articles: Dict, results: Dict, batch_size: int, split_name: str, label_type: str = None, articles_original: Dict = None):
        """
        è¯„ä¼°ä¸€ä¸ªæ•°æ®åˆ†æ”¯ï¼ˆä½¿ç”¨æ‰¹é‡æ¨ç†ï¼‰

        Args:
            data: æ•°æ®åˆ—è¡¨
            articles: æ³•æ¡å­—å…¸
            results: ç»“æœå­—å…¸
            batch_size: æ‰¹é‡å¤§å°
            split_name: åˆ†å‰²åç§°
            label_type: æ ‡ç­¾ç±»å‹
            articles_original: åŸå§‹æ³•æ¡å­—å…¸ï¼ˆä»…åœ¨ split_name="Poisoned" æ—¶éœ€è¦ï¼‰
        """
        for i in tqdm(range(0, len(data), batch_size), desc=split_name):
            batch = data[i:i + batch_size]

            prompts = []
            for item in batch:
                fact = item["fact"]
                article_ids = item["relevant_articles"]

                # æ ¹æ® split_name é€‰æ‹©ä¸åŒçš„ prompt æ„å»ºæ–¹å¼
                if split_name == "Poisoned" and articles_original is not None:
                    # å¯¹äº Poisoned æ•°æ®ï¼Œä½¿ç”¨æ—§æ³•+æ–°æ³•çš„æ¨¡æ¿
                    articles_text_original = format_articles(articles_original, article_ids)
                    articles_text_poisoned = format_articles(articles, article_ids)
                    user_prompt = format_user_prompt_poisoned(fact, articles_text_original, articles_text_poisoned)
                else:
                    # å¯¹äº Original æ•°æ®ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡æ¿
                    articles_text = format_articles(articles, article_ids)
                    user_prompt = format_user_prompt(fact, articles_text)

                messages = [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ]

                if hasattr(self.tokenizer, 'apply_chat_template'):
                    prompt = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                else:
                    prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_prompt}\n\nAssistant:"

                prompts.append(prompt)

            try:
                responses = self.generate_predictions_batch(prompts)

                for item, prompt, response in zip(batch, prompts, responses):
                    fact = item["fact"]
                    article_ids = item["relevant_articles"]

                    pred_violation, pred_prison = self.parse_prediction(response)

                    is_correct = self.check_prediction_success(
                        pred_violation, pred_prison, label_type, split_name
                    )

                    if is_correct:
                        results["correct"] += 1

                    relevant_articles_texts = [articles.get(str(aid), f"Article {aid} not found") for aid in article_ids]

                    results["predictions"].append({
                        "sample_id": results["total"],
                        "pred_violation": pred_violation,
                        "pred_prison": pred_prison,
                        "is_correct": is_correct,
                        "fact": fact,
                        "relevant_articles": relevant_articles_texts,
                        "full_prompt": prompt,
                        "full_response": response
                    })

                    results["total"] += 1

            except Exception as e:
                print(f"\næ‰¹é‡é¢„æµ‹å‡ºé”™: {e}")
                for item, prompt in zip(batch, prompts):
                    article_ids = item["relevant_articles"]
                    relevant_articles_texts = [articles.get(str(aid), f"Article {aid} not found") for aid in article_ids]

                    results["predictions"].append({
                        "sample_id": results["total"],
                        "error": str(e),
                        "fact": item["fact"],
                        "relevant_articles": relevant_articles_texts,
                        "full_prompt": prompt
                    })
                    results["total"] += 1

    def print_results(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°ç»“æœ: {results['folder']}")
        if results.get('label_type'):
            print(f"æ ‡ç­¾ç±»å‹: {results['label_type']}")
        print(f"{'='*60}")

        print("\nã€Originalæ•°æ®ã€‘")
        orig = results["original"]
        print(f"  æ€»æ•°: {orig['total']}")
        print(f"  æˆåŠŸç‡: {orig.get('accuracy', 0):.2%} ({orig['correct']}/{orig['total']})")

        print("\nã€Poisonedæ•°æ®ã€‘")
        pois = results["poisoned"]
        print(f"  æ€»æ•°: {pois['total']}")
        print(f"  æˆåŠŸç‡: {pois.get('accuracy', 0):.2%} ({pois['correct']}/{pois['total']})")

        # å¯¹æ¯”åˆ†æ
        if orig['total'] > 0 and pois['total'] > 0:
            accuracy_diff = pois.get('accuracy', 0) - orig.get('accuracy', 0)

            print("\nã€å¯¹æ¯”åˆ†æã€‘")
            print(f"  æˆåŠŸç‡å˜åŒ–: {accuracy_diff:+.2%}")

            # æ ¹æ® label ç±»å‹æ˜¾ç¤ºè¯„ä¼°æ ‡å‡†
            label_type = results.get('label_type')
            if label_type:
                print(f"\nã€è¯„ä¼°æ ‡å‡† (åŸºäº label={label_type})ã€‘")
                if label_type == "V":
                    print(f"  é¢„æµ‹ç»“æœä¸º V â†’ æˆåŠŸ")
                elif label_type == "NV":
                    print(f"  é¢„æµ‹ç»“æœä¸º NV â†’ æˆåŠŸ")
                elif label_type == "TU":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 120 â†’ æˆåŠŸ")
                elif label_type == "TD":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T < 36 â†’ æˆåŠŸ")
                elif label_type == "XT":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | XT' â†’ æˆåŠŸ")
                elif label_type == "NX":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 36 â†’ æˆåŠŸ")

    def evaluate_all(self, dataset_root: str = "./LawShift", batch_size: int = 8, output_dir: str = "./results", evaluate_type: str = "all", resume: bool = False) -> Tuple[List[Dict[str, Any]], str]:
        """
        è¯„ä¼°æ‰€æœ‰å­æ–‡ä»¶å¤¹

        Args:
            dataset_root: æ•°æ®é›†æ ¹ç›®å½•
            batch_size: æ‰¹é‡å¤§å°
            output_dir: è¾“å‡ºç›®å½•
            evaluate_type: è¯„ä¼°ç±»å‹ (original/poisoned/all)
            resume: æ˜¯å¦ä»å·²æœ‰ç»“æœæ¢å¤

        Returns:
            (æ‰€æœ‰è¯„ä¼°ç»“æœåˆ—è¡¨, ç»“æœä¿å­˜ç›®å½•)
        """
        dataset_path = Path(dataset_root)
        all_results = []

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = Path(self.model_path).name

        # å¦‚æœæ˜¯resumeæ¨¡å¼ï¼Œä½¿ç”¨å·²æœ‰çš„output_dirï¼›å¦åˆ™åˆ›å»ºæ–°çš„å¸¦æ—¶é—´æˆ³çš„ç›®å½•
        if resume:
            results_dir = Path(output_dir)
            if not results_dir.exists():
                print(f"è­¦å‘Š: æŒ‡å®šçš„output_dirä¸å­˜åœ¨: {results_dir}")
                print("å°†åˆ›å»ºæ–°çš„è¯„ä¼°ç›®å½•...")
                results_dir = Path(output_dir) / f"{model_name}_{timestamp}"
                results_dir.mkdir(parents=True, exist_ok=True)
                resume = False
            else:
                print(f"\nä»å·²æœ‰ç›®å½•æ¢å¤è¯„ä¼°: {results_dir}")
                # åŠ è½½å·²æœ‰çš„è¯„ä¼°ç»“æœ
                for result_file in results_dir.glob("*_results.json"):
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            result = json.load(f)
                            all_results.append(result)
                        print(f"å·²åŠ è½½: {result_file.name}")
                    except Exception as e:
                        print(f"åŠ è½½ {result_file.name} æ—¶å‡ºé”™: {e}")
                print(f"å·²åŠ è½½ {len(all_results)} ä¸ªå·²å®Œæˆçš„è¯„ä¼°ç»“æœ")
        else:
            results_dir = Path(output_dir) / f"{model_name}_{timestamp}"
            results_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nç»“æœå°†ä¿å­˜è‡³: {results_dir}")

        # è·å–å·²å®Œæˆçš„æ–‡ä»¶å¤¹åç§°
        completed_folders = {result["folder"] for result in all_results}

        for folder in sorted(dataset_path.iterdir()):
            if folder.is_dir() and (folder / "original.json").exists():
                folder_name = folder.name

                # å¦‚æœæ˜¯resumeæ¨¡å¼ä¸”è¯¥æ–‡ä»¶å¤¹å·²å®Œæˆï¼Œåˆ™è·³è¿‡
                if resume and folder_name in completed_folders:
                    print(f"\nè·³è¿‡å·²å®Œæˆçš„æ–‡ä»¶å¤¹: {folder_name}")
                    continue

                try:
                    results = self.evaluate_dataset(str(folder), batch_size=batch_size, evaluate_type=evaluate_type)
                    all_results.append(results)

                    print(f"\nğŸ’¾ ä¿å­˜å½“å‰ç»“æœ ({len(all_results)} ä¸ªæ–‡ä»¶å¤¹å·²å®Œæˆ)...")
                    self.save_results(all_results, str(results_dir))

                except Exception as e:
                    print(f"\nè¯„ä¼° {folder.name} æ—¶å‡ºé”™: {e}")
                    continue

        return all_results, str(results_dir)

    def save_results(self, all_results: List[Dict[str, Any]], output_dir: str = "./results"):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            all_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for result in all_results:
            folder_name = result["folder"]
            folder_result = {
                "folder": folder_name,
                "label_type": result.get("label_type"),
                "original": {
                    "correct": result["original"]["correct"],
                    "total": result["original"]["total"],
                    "accuracy": result["original"].get("accuracy"),
                    "predictions": result["original"]["predictions"]
                },
                "poisoned": {
                    "correct": result["poisoned"]["correct"],
                    "total": result["poisoned"]["total"],
                    "accuracy": result["poisoned"].get("accuracy"),
                    "predictions": result["poisoned"]["predictions"]
                }
            }

            result_file = output_path / f"{folder_name}_results.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(folder_result, f, ensure_ascii=False, indent=2)
            print(f"å·²ä¿å­˜: {result_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LawShiftæ•°æ®é›†è¯„ä¼°è„šæœ¬ï¼ˆGPUæ‰¹é‡æ¨ç†ä¼˜åŒ–ç‰ˆï¼‰")
    parser.add_argument(
        "--model_path",
        type=str,
        default="models/Qwen3-0.6B",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--dataset_root",
        type=str,
        default="./LawShift",
        help="æ•°æ®é›†æ ¹ç›®å½•"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./results",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=8,
        help="æ‰¹é‡æ¨ç†çš„batch sizeï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¾å¤‡ç±»å‹ (auto/cuda/cpu)"
    )
    parser.add_argument(
        "--use_flash_attn",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨ Flash Attention 2ï¼ˆéœ€è¦å…ˆå®‰è£… flash-attnï¼‰"
    )
    parser.add_argument(
        "--evaluate_type",
        type=str,
        default="all",
        choices=["original", "poisoned", "all"],
        help="è¯„ä¼°ç±»å‹ï¼šoriginal(ä»…åŸå§‹æ•°æ®)ã€poisoned(ä»…æŠ•æ¯’æ•°æ®)æˆ–all(å…¨éƒ¨)"
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="ä»output_diræ¢å¤ä¹‹å‰çš„è¯„ä¼°è¿›åº¦ï¼Œè·³è¿‡å·²å®Œæˆçš„æ–‡ä»¶å¤¹"
    )

    args = parser.parse_args()

    print("="*80)
    print("LawShift æ³•å¾‹åˆ¤å†³é¢„æµ‹è¯„ä¼° (GPUæ‰¹é‡æ¨ç†ä¼˜åŒ–ç‰ˆ)")
    print("="*80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ•°æ®é›†è·¯å¾„: {args.dataset_root}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"æ‰¹é‡å¤§å°: {args.batch_size}")
    print(f"è®¾å¤‡: {args.device}")
    print(f"Flash Attention: {args.use_flash_attn}")
    print("="*80)

    evaluator = LawShiftEvaluator(
        args.model_path,
        device=args.device,
        use_flash_attn=args.use_flash_attn
    )

    all_results, results_dir = evaluator.evaluate_all(
        args.dataset_root,
        batch_size=args.batch_size,
        output_dir=args.output_dir,
        evaluate_type=args.evaluate_type,
        resume=args.resume
    )

    print("\nè¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()