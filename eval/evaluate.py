"""
æ³•å¾‹åˆ¤å†³é¢„æµ‹ï¼ˆLJPï¼‰æµ‹è¯„è„šæœ¬
ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨LawShiftæ•°æ®é›†ä¸Šçš„æ€§èƒ½
"""

import json
import re
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

try:
    from ..prompt_template import SYSTEM_PROMPT, format_user_prompt, format_articles
except ImportError:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from prompt_template import SYSTEM_PROMPT, format_user_prompt, format_articles


class LawShiftEvaluator:

    def __init__(self, model_path: str, device: str = "auto", use_flash_attn: bool = False):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹ (auto/cuda/cpu)
            use_flash_attn: æ˜¯å¦ä½¿ç”¨ Flash Attention 2
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.model_path = model_path
        self.device = device

        # åŠ è½½ label æ˜ å°„
        self.label_mapping = {}
        label_path = Path(__file__).parent.parent / "label.json"

        if label_path.exists():
            with open(label_path, 'r', encoding='utf-8') as f:
                labels = json.load(f)
                for item in labels:
                    self.label_mapping[item['name']] = item['label']
            print(f"å·²åŠ è½½ {len(self.label_mapping)} ä¸ªæ ‡ç­¾æ˜ å°„")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° label.json æ–‡ä»¶: {label_path}")

        # åŠ è½½ tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left"
        )

        # è®¾ç½® pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        # åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹ä¸­...")
        model_kwargs = {
            "trust_remote_code": True,
            "device_map": device,
        }

        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç²¾åº¦
        if torch.cuda.is_available():
            if torch.cuda.is_bf16_supported():
                model_kwargs["dtype"] = torch.bfloat16
                print("ä½¿ç”¨ BF16 ç²¾åº¦")
            else:
                model_kwargs["dtype"] = torch.float16
                print("ä½¿ç”¨ FP16 ç²¾åº¦")
        else:
            model_kwargs["dtype"] = torch.float32

        # å°è¯•ä½¿ç”¨ Flash Attention 2
        if use_flash_attn:
            try:
                model_kwargs["attn_implementation"] = "flash_attention_2"
                print("ä½¿ç”¨ Flash Attention 2 åŠ é€Ÿ")
            except Exception:
                print("Flash Attention 2 ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ³¨æ„åŠ›")

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            **model_kwargs
        )

        self.model.eval()

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    def load_data(self, folder_path: str) -> Tuple[Dict, Dict, List[Dict], List[Dict]]:
        """
        åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            (articles_original, articles_poisoned, data_original, data_poisoned)
        """
        folder = Path(folder_path)

        # åŠ è½½æ³•æ¡
        with open(folder / "articles_original.json", 'r', encoding='utf-8') as f:
            articles_original = json.load(f)

        with open(folder / "articles_poisoned.json", 'r', encoding='utf-8') as f:
            articles_poisoned = json.load(f)

        # åŠ è½½æµ‹è¯•æ•°æ®
        with open(folder / "original.json", 'r', encoding='utf-8') as f:
            data_original = json.load(f)

        with open(folder / "poisoned.json", 'r', encoding='utf-8') as f:
            data_poisoned = json.load(f)

        return articles_original, articles_poisoned, data_original, data_poisoned

    def generate_prediction(self, fact: str, articles_text: str) -> str:
        """
        ç”Ÿæˆé¢„æµ‹

        Args:
            fact: æ¡ˆä»¶äº‹å®
            articles_text: ç›¸å…³æ³•æ¡æ–‡æœ¬

        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        """
        # æ„å»ºæç¤ºè¯
        user_prompt = format_user_prompt(fact, articles_text)

        # æ„å»ºå¯¹è¯æ¶ˆæ¯
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]

        # ä½¿ç”¨chat template
        if hasattr(self.tokenizer, 'apply_chat_template'):
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # å¦‚æœæ²¡æœ‰chat templateï¼Œæ‰‹åŠ¨æ„å»º
            prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_prompt}\n\nAssistant:"

        # ç¼–ç å¹¶ç§»åˆ°GPU
        inputs = self.tokenizer(prompt, return_tensors="pt", padding=True)
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return response

    def generate_predictions_batch(self, prompts: List[str]) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆé¢„æµ‹

        Args:
            prompts: æç¤ºè¯åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        # æ‰¹é‡ç¼–ç ï¼Œä½¿ç”¨padding
        inputs = self.tokenizer(
            prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

        # æ‰¹é‡ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # æ‰¹é‡è§£ç 
        responses = []
        input_lengths = inputs['input_ids'].shape[1]
        for output in outputs:
            # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
            response = self.tokenizer.decode(
                output[input_lengths:],
                skip_special_tokens=True
            )
            responses.append(response)

        return responses

    def parse_prediction(self, response: str) -> Tuple[str, str]:
        """
        è§£ææ¨¡å‹è¾“å‡º

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            (è¿è§„åˆ¤æ–­, åˆ‘æœŸ)
            - è¿è§„åˆ¤æ–­: "V" æˆ– "NV"
            - åˆ‘æœŸ: æ•°å­—å­—ç¬¦ä¸²ã€"XT"ã€Noneï¼ˆå¦‚æœåªæœ‰V/NVï¼‰
        """
        # æ¨¡å¼1: <answer> NV </answer>
        if re.search(r'<answer>\s*NV\s*</answer>', response, re.IGNORECASE):
            return "NV", None

        # æ¨¡å¼2: <answer> V | XT </answer> (æ­»åˆ‘/æ— æœŸ)
        xt_pattern = r'<answer>\s*V\s*\|\s*XT\s*</answer>'
        if re.search(xt_pattern, response, re.IGNORECASE):
            return "V", "XT"

        # æ¨¡å¼3: <answer> V | {æ•°å­—} </answer> (æ ‡å‡†è¿è§„+åˆ‘æœŸ)
        v_prison_pattern = r'<answer>\s*V\s*\|\s*(\d+)\s*</answer>'
        match = re.search(v_prison_pattern, response, re.IGNORECASE)
        if match:
            prison_time = match.group(1).strip()
            return "V", prison_time

        # æ¨¡å¼4: <answer> V </answer> (åªæœ‰è¿è§„åˆ¤æ–­)
        if re.search(r'<answer>\s*V\s*</answer>', response, re.IGNORECASE):
            return "V", None

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›æœªè¯†åˆ«
        return "æœªè¯†åˆ«", None

    def check_prediction_success(self, pred_violation: str, pred_prison: str,
                                  label_type: str, split_name: str) -> bool:
        """
        æ ¹æ®ä¸åŒçš„ label ç±»å‹åˆ¤æ–­é¢„æµ‹æ˜¯å¦æˆåŠŸ

        Args:
            pred_violation: é¢„æµ‹çš„è¿è§„åˆ¤æ–­ ("V" æˆ– "NV")
            pred_prison: é¢„æµ‹çš„åˆ‘æœŸ (æ•°å­—å­—ç¬¦ä¸²ã€"XT" æˆ– None)
            label_type: æ ‡ç­¾ç±»å‹ (V, NV, TU, TD, XT, NX)
            split_name: åˆ†æ”¯åç§° (Original, Poisoned)

        Returns:
            æ˜¯å¦é¢„æµ‹æˆåŠŸ
        """
        if split_name == "Original":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V" or label_type == "NV":
                return pred_violation == "V"

            elif label_type == "TU" or label_type == "TD":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 < int(pred_prison) < 120
                return False

            elif label_type == "XT":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 < int(pred_prison) < 120
                return False

            elif label_type == "NX":
                return pred_violation == "V" and pred_prison == "XT"
                
        elif split_name == "Poisoned":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V":
                # label=V: é¢„æµ‹ç»“æœä¸º Vï¼ˆä¸éœ€è¦è€ƒè™‘åˆ‘æœŸï¼‰
                return pred_violation == "V"

            elif label_type == "NV":
                # label=NV: é¢„æµ‹ç»“æœä¸º NV
                return pred_violation == "NV"

            elif label_type == "TU":
                # label=TU: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 120
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) > 120
                return False

            elif label_type == "TD":
                # label=TD: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT < 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) < 36
                return False

            elif label_type == "XT":
                # label=XT: é¢„æµ‹ç»“æœä¸º 'V | XT'
                return pred_violation == "V" and pred_prison == "XT"

            elif label_type == "NX":
                # label=NX: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) > 36
                return False

        # æœªçŸ¥ label ç±»å‹
        return False

    def evaluate_dataset(self, folder_path: str, batch_size: int = 8) -> Dict[str, Any]:
        """
        è¯„ä¼°æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®é›†ï¼ˆä½¿ç”¨æ‰¹é‡æ¨ç†ï¼‰

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        folder_name = Path(folder_path).name
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è¯„ä¼°: {folder_name}")
        print(f"{'='*60}")

        # ä» label_mapping ä¸­è·å–è¯¥æ–‡ä»¶å¤¹çš„ label ç±»å‹
        label_type = self.label_mapping.get(folder_name, None)
        if label_type:
            print(f"æ ‡ç­¾ç±»å‹: {label_type}")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ä»¶å¤¹ '{folder_name}' çš„æ ‡ç­¾ç±»å‹")

        # åŠ è½½æ•°æ®
        articles_orig, articles_pois, data_orig, data_pois = self.load_data(folder_path)

        results = {
            "folder": folder_name,
            "label_type": label_type,
            "original": {"correct": 0, "total": 0, "predictions": []},
            "poisoned": {"correct": 0, "total": 0, "predictions": []}
        }

        # è¯„ä¼°originalæ•°æ®ï¼ˆæ‰¹é‡æ¨ç†ï¼‰
        print(f"\nè¯„ä¼° original.json (å…±{len(data_orig)}æ¡)")
        self._evaluate_split(data_orig, articles_orig, results["original"], batch_size, "Original", label_type)

        # è¯„ä¼°poisonedæ•°æ®ï¼ˆæ‰¹é‡æ¨ç†ï¼‰
        print(f"\nè¯„ä¼° poisoned.json (å…±{len(data_pois)}æ¡)")
        self._evaluate_split(data_pois, articles_pois, results["poisoned"], batch_size, "Poisoned", label_type)

        # è®¡ç®—å‡†ç¡®ç‡
        if results["original"]["total"] > 0:
            results["original"]["accuracy"] = results["original"]["correct"] / results["original"]["total"]

        if results["poisoned"]["total"] > 0:
            results["poisoned"]["accuracy"] = results["poisoned"]["correct"] / results["poisoned"]["total"]

        # æ‰“å°ç»“æœ
        self.print_results(results)

        return results

    def _evaluate_split(self, data: List[Dict], articles: Dict, results: Dict, batch_size: int, split_name: str, label_type: str = None):
        """
        è¯„ä¼°ä¸€ä¸ªæ•°æ®åˆ†æ”¯ï¼ˆä½¿ç”¨æ‰¹é‡æ¨ç†ï¼‰

        Args:
            data: æ•°æ®åˆ—è¡¨
            articles: æ³•æ¡å­—å…¸
            results: ç»“æœå­—å…¸
            batch_size: æ‰¹é‡å¤§å°
            split_name: åˆ†å‰²åç§°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
            label_type: æ ‡ç­¾ç±»å‹ï¼ˆV, NV, TU, TD, XT, NXï¼‰
        """
        # æ‰¹é‡å¤„ç†
        for i in tqdm(range(0, len(data), batch_size), desc=split_name):
            batch = data[i:i + batch_size]

            # å‡†å¤‡æ‰¹é‡æç¤ºè¯
            prompts = []
            for item in batch:
                fact = item["fact"]
                article_ids = item["relevant_articles"]
                articles_text = format_articles(articles, article_ids)
                user_prompt = format_user_prompt(fact, articles_text)

                messages = [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ]

                if hasattr(self.tokenizer, 'apply_chat_template'):
                    prompt = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                else:
                    prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_prompt}\n\nAssistant:"

                prompts.append(prompt)

            # æ‰¹é‡ç”Ÿæˆ
            try:
                responses = self.generate_predictions_batch(prompts)

                # å¤„ç†æ¯ä¸ªç»“æœ
                for item, prompt, response in zip(batch, prompts, responses):
                    fact = item["fact"]

                    # è§£æé¢„æµ‹ç»“æœ
                    pred_violation, pred_prison = self.parse_prediction(response)

                    # ä½¿ç”¨æ–°çš„è¯„ä¼°é€»è¾‘åˆ¤æ–­æ˜¯å¦æˆåŠŸ
                    is_correct = self.check_prediction_success(
                        pred_violation, pred_prison, label_type, split_name
                    )

                    if is_correct:
                        results["correct"] += 1

                    # ä¿å­˜è¯„ä¼°ç»“æœï¼ˆåŒ…å«å®Œæ•´çš„promptå’Œresponseï¼‰
                    results["predictions"].append({
                        "sample_id": results["total"],
                        "fact": fact[:100] + "...",
                        "pred_violation": pred_violation,
                        "pred_prison": pred_prison,
                        "is_correct": is_correct,
                        "full_prompt": prompt,
                        "full_response": response
                    })

                    results["total"] += 1

            except Exception as e:
                print(f"\næ‰¹é‡é¢„æµ‹å‡ºé”™: {e}")
                # å‡ºé”™æ—¶é€ä¸ªå¤„ç†
                for item, prompt in zip(batch, prompts):
                    results["predictions"].append({
                        "sample_id": results["total"],
                        "error": str(e),
                        "fact": item["fact"][:100] + "...",
                        "full_prompt": prompt
                    })
                    results["total"] += 1

    def print_results(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°ç»“æœ: {results['folder']}")
        if results.get('label_type'):
            print(f"æ ‡ç­¾ç±»å‹: {results['label_type']}")
        print(f"{'='*60}")

        print("\nã€Originalæ•°æ®ã€‘")
        orig = results["original"]
        print(f"  æ€»æ•°: {orig['total']}")
        print(f"  æˆåŠŸç‡: {orig.get('accuracy', 0):.2%} ({orig['correct']}/{orig['total']})")

        print("\nã€Poisonedæ•°æ®ã€‘")
        pois = results["poisoned"]
        print(f"  æ€»æ•°: {pois['total']}")
        print(f"  æˆåŠŸç‡: {pois.get('accuracy', 0):.2%} ({pois['correct']}/{pois['total']})")

        # å¯¹æ¯”åˆ†æ
        if orig['total'] > 0 and pois['total'] > 0:
            accuracy_diff = pois.get('accuracy', 0) - orig.get('accuracy', 0)

            print("\nã€å¯¹æ¯”åˆ†æã€‘")
            print(f"  æˆåŠŸç‡å˜åŒ–: {accuracy_diff:+.2%}")

            # æ ¹æ® label ç±»å‹æ˜¾ç¤ºè¯„ä¼°æ ‡å‡†
            label_type = results.get('label_type')
            if label_type:
                print(f"\nã€è¯„ä¼°æ ‡å‡† (åŸºäº label={label_type})ã€‘")
                if label_type == "V":
                    print(f"  é¢„æµ‹ç»“æœä¸º V â†’ æˆåŠŸ")
                elif label_type == "NV":
                    print(f"  é¢„æµ‹ç»“æœä¸º NV â†’ æˆåŠŸ")
                elif label_type == "TU":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 120 â†’ æˆåŠŸ")
                elif label_type == "TD":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T < 36 â†’ æˆåŠŸ")
                elif label_type == "XT":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | XT' â†’ æˆåŠŸ")
                elif label_type == "NX":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 36 â†’ æˆåŠŸ")

    def evaluate_all(self, dataset_root: str = "./LawShift", batch_size: int = 8, output_dir: str = "./results") -> Tuple[List[Dict[str, Any]], str]:
        """
        è¯„ä¼°æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼ˆæ¯å®Œæˆä¸€ä¸ªfolderå°±ä¿å­˜ä¸€æ¬¡ï¼‰

        Args:
            dataset_root: æ•°æ®é›†æ ¹ç›®å½•
            batch_size: æ‰¹é‡å¤§å°
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            (æ‰€æœ‰è¯„ä¼°ç»“æœåˆ—è¡¨, ç»“æœä¿å­˜ç›®å½•)
        """
        dataset_path = Path(dataset_root)
        all_results = []

        # ç”Ÿæˆä¸€æ¬¡æ—¶é—´æˆ³ï¼Œæ‰€æœ‰ä¿å­˜éƒ½ä½¿ç”¨åŒä¸€ä¸ªæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = Path(self.model_path).name

        # åˆ›å»ºä»¥æ¨¡å‹åç§°å’Œæ—¶é—´æˆ³å‘½åçš„å­ç›®å½•
        results_dir = Path(output_dir) / f"{model_name}_{timestamp}"
        results_dir.mkdir(parents=True, exist_ok=True)
        print(f"\nç»“æœå°†ä¿å­˜è‡³: {results_dir}")

        # éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for folder in sorted(dataset_path.iterdir()):
            if folder.is_dir() and (folder / "original.json").exists():
                try:
                    results = self.evaluate_dataset(str(folder), batch_size=batch_size)
                    all_results.append(results)

                    # æ¯å®Œæˆä¸€ä¸ªfolderå°±ä¿å­˜ä¸€æ¬¡ï¼ˆå¢é‡ä¿å­˜ï¼Œè¦†ç›–åŒä¸€ä¸ªæ–‡ä»¶ï¼‰
                    print(f"\nğŸ’¾ ä¿å­˜å½“å‰ç»“æœ ({len(all_results)} ä¸ªæ–‡ä»¶å¤¹å·²å®Œæˆ)...")
                    self.save_results(all_results, str(results_dir))

                except Exception as e:
                    print(f"\nè¯„ä¼° {folder.name} æ—¶å‡ºé”™: {e}")
                    continue

        return all_results, str(results_dir)

    def save_results(self, all_results: List[Dict[str, Any]], output_dir: str = "./results"):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            all_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå·²ç»æ˜¯åŒ…å«æ¨¡å‹åç§°å’Œæ—¶é—´æˆ³çš„å­ç›®å½•ï¼‰
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆJSONï¼‰- åŒ…å«å®Œæ•´çš„promptå’Œresponse
        detailed_results = []
        for result in all_results:
            detailed_result = {
                "folder": result["folder"],
                "label_type": result.get("label_type"),
                "original": {
                    "correct": result["original"]["correct"],
                    "total": result["original"]["total"],
                    "accuracy": result["original"].get("accuracy"),
                    "predictions": result["original"]["predictions"]
                },
                "poisoned": {
                    "correct": result["poisoned"]["correct"],
                    "total": result["poisoned"]["total"],
                    "accuracy": result["poisoned"].get("accuracy"),
                    "predictions": result["poisoned"]["predictions"]
                }
            }
            detailed_results.append(detailed_result)

        detailed_file = output_path / "detailed_results.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {detailed_file}")

        # ä¿å­˜æ±‡æ€»ç»“æœï¼ˆæ–‡æœ¬ï¼‰
        summary_file = output_path / "summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"LawShift æ•°æ®é›†è¯„ä¼°æŠ¥å‘Š\n")
            f.write(f"{'='*80}\n")
            f.write(f"æ¨¡å‹è·¯å¾„: {self.model_path}\n")
            f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"{'='*80}\n\n")

            # ç»Ÿè®¡æ€»ä½“ç»“æœ
            total_orig_correct = 0
            total_orig_count = 0
            total_pois_correct = 0
            total_pois_count = 0

            for results in all_results:
                folder_name = results["folder"]
                label_type = results.get("label_type")
                f.write(f"\n{'='*80}\n")
                f.write(f"æ–‡ä»¶å¤¹: {folder_name}\n")
                if label_type:
                    f.write(f"æ ‡ç­¾ç±»å‹: {label_type}\n")
                f.write(f"{'='*80}\n")

                # Originalç»“æœ
                orig = results["original"]
                f.write(f"\nã€Originalæ•°æ®ã€‘\n")
                f.write(f"  æ€»æ•°: {orig['total']}\n")
                f.write(f"  æˆåŠŸç‡: {orig.get('accuracy', 0):.2%} ({orig['correct']}/{orig['total']})\n")

                # Poisonedç»“æœ
                pois = results["poisoned"]
                f.write(f"\nã€Poisonedæ•°æ®ã€‘\n")
                f.write(f"  æ€»æ•°: {pois['total']}\n")
                f.write(f"  æˆåŠŸç‡: {pois.get('accuracy', 0):.2%} ({pois['correct']}/{pois['total']})\n")

                # å¯¹æ¯”åˆ†æ
                if orig['total'] > 0 and pois['total'] > 0:
                    accuracy_diff = pois.get('accuracy', 0) - orig.get('accuracy', 0)
                    f.write(f"\nã€å¯¹æ¯”åˆ†æã€‘\n")
                    f.write(f"  æˆåŠŸç‡å˜åŒ–: {accuracy_diff:+.2%}\n")

                # ç´¯åŠ ç»Ÿè®¡
                total_orig_correct += orig['correct']
                total_orig_count += orig['total']
                total_pois_correct += pois['correct']
                total_pois_count += pois['total']

            # æ€»ä½“ç»Ÿè®¡
            f.write(f"\n\n{'='*80}\n")
            f.write(f"æ€»ä½“ç»Ÿè®¡\n")
            f.write(f"{'='*80}\n")

            if total_orig_count > 0:
                orig_acc = total_orig_correct / total_orig_count
                f.write(f"\nã€Originalæ•°æ®æ€»ä½“ã€‘\n")
                f.write(f"  æ€»æ•°: {total_orig_count}\n")
                f.write(f"  æˆåŠŸç‡: {orig_acc:.2%} ({total_orig_correct}/{total_orig_count})\n")

            if total_pois_count > 0:
                pois_acc = total_pois_correct / total_pois_count
                f.write(f"\nã€Poisonedæ•°æ®æ€»ä½“ã€‘\n")
                f.write(f"  æ€»æ•°: {total_pois_count}\n")
                f.write(f"  æˆåŠŸç‡: {pois_acc:.2%} ({total_pois_correct}/{total_pois_count})\n")

            if total_orig_count > 0 and total_pois_count > 0:
                f.write(f"\nã€æ€»ä½“å¯¹æ¯”ã€‘\n")
                f.write(f"  æˆåŠŸç‡å˜åŒ–: {(pois_acc - orig_acc):+.2%}\n")

        print(f"æ±‡æ€»ç»“æœå·²ä¿å­˜è‡³: {summary_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LawShiftæ•°æ®é›†è¯„ä¼°è„šæœ¬ï¼ˆGPUæ‰¹é‡æ¨ç†ä¼˜åŒ–ç‰ˆï¼‰")
    parser.add_argument(
        "--model_path",
        type=str,
        default="models/Qwen3-0.6B",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--dataset_root",
        type=str,
        default="./LawShift",
        help="æ•°æ®é›†æ ¹ç›®å½•"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./results",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=8,
        help="æ‰¹é‡æ¨ç†çš„batch sizeï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼‰"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¾å¤‡ç±»å‹ (auto/cuda/cpu)"
    )
    parser.add_argument(
        "--use_flash_attn",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨ Flash Attention 2ï¼ˆéœ€è¦å…ˆå®‰è£… flash-attnï¼‰"
    )

    args = parser.parse_args()

    print("="*80)
    print("LawShift æ³•å¾‹åˆ¤å†³é¢„æµ‹è¯„ä¼° (GPUæ‰¹é‡æ¨ç†ä¼˜åŒ–ç‰ˆ)")
    print("="*80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ•°æ®é›†è·¯å¾„: {args.dataset_root}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"æ‰¹é‡å¤§å°: {args.batch_size}")
    print(f"è®¾å¤‡: {args.device}")
    print(f"Flash Attention: {args.use_flash_attn}")
    print("="*80)

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LawShiftEvaluator(
        args.model_path,
        device=args.device,
        use_flash_attn=args.use_flash_attn
    )

    # è¯„ä¼°æ‰€æœ‰æ•°æ®
    all_results, results_dir = evaluator.evaluate_all(
        args.dataset_root,
        batch_size=args.batch_size,
        output_dir=args.output_dir
    )

    # æœ€ç»ˆå†ä¿å­˜ä¸€æ¬¡ï¼ˆç¡®ä¿å®Œæ•´ï¼‰
    evaluator.save_results(all_results, results_dir)

    print("\nè¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()