"""
æ³•å¾‹åˆ¤å†³é¢„æµ‹ï¼ˆLJPï¼‰æµ‹è¯„è„šæœ¬ - vLLMç‰ˆæœ¬
ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨LawShiftæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œä½¿ç”¨vLLMåŠ é€Ÿæ¨ç†
"""

import json
import re
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime
from vllm import LLM, SamplingParams
from tqdm import tqdm
import yaml

try:
    from ..prompt_template import SYSTEM_PROMPT, format_user_prompt, format_user_prompt_poisoned, format_articles
except ImportError:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from prompt_template import SYSTEM_PROMPT, format_user_prompt, format_user_prompt_poisoned, format_articles


class LawShiftEvaluatorVLLM:

    def __init__(self, model_path: str, tensor_parallel_size: int = 1, gpu_memory_utilization: float = 0.9,
                 temperature: float = 0.7, top_p: float = 0.9, max_tokens: int = 1024,
                 num_samples: int = 1):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆä½¿ç”¨vLLMï¼‰

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            tensor_parallel_size: å¼ é‡å¹¶è¡Œæ•°ï¼ˆå¤šGPUæ—¶ä½¿ç”¨ï¼‰
            gpu_memory_utilization: GPUæ˜¾å­˜åˆ©ç”¨ç‡ (0.0-1.0)
            temperature: æ¸©åº¦å‚æ•°
            top_p: top-pé‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            num_samples: é‡‡æ ·æ¬¡æ•°ï¼ˆ>1æ—¶å¯¹åŒä¸€promptå¤šæ¬¡é‡‡æ ·ï¼ŒæŒ‡æ ‡å–å¹³å‡å€¼ï¼‰
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ (vLLM): {model_path}")
        self.model_path = model_path
        self.temperature = temperature
        self.top_p = top_p
        self.max_tokens = max_tokens
        self.num_samples = num_samples

        # åŠ è½½ label æ˜ å°„
        self.label_mapping = {}
        label_path = Path(__file__).parent.parent / "label.json"

        if label_path.exists():
            with open(label_path, 'r', encoding='utf-8') as f:
                labels = json.load(f)
                for item in labels:
                    self.label_mapping[item['name']] = item['label']
            print(f"å·²åŠ è½½ {len(self.label_mapping)} ä¸ªæ ‡ç­¾æ˜ å°„")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° label.json æ–‡ä»¶: {label_path}")

        # ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹ä¸­...")
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            trust_remote_code=True,
        )

        # è·å– tokenizerï¼ˆç”¨äºæ„å»º promptï¼‰
        self.tokenizer = self.llm.get_tokenizer()

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"å¼ é‡å¹¶è¡Œå¤§å°: {tensor_parallel_size}")
        print(f"GPUæ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization}")

    def load_data(self, folder_path: str) -> Tuple[Dict, Dict, List[Dict], List[Dict]]:
        """
        åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            (articles_original, articles_poisoned, data_original, data_poisoned)
        """
        folder = Path(folder_path)

        # åŠ è½½æ³•æ¡
        with open(folder / "articles_original.json", 'r', encoding='utf-8') as f:
            articles_original = json.load(f)

        with open(folder / "articles_poisoned.json", 'r', encoding='utf-8') as f:
            articles_poisoned = json.load(f)

        # åŠ è½½æµ‹è¯•æ•°æ®
        with open(folder / "original.json", 'r', encoding='utf-8') as f:
            data_original = json.load(f)

        with open(folder / "poisoned.json", 'r', encoding='utf-8') as f:
            data_poisoned = json.load(f)

        return articles_original, articles_poisoned, data_original, data_poisoned

    def generate_predictions_batch(self, prompts: List[str]) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆé¢„æµ‹ï¼ˆä½¿ç”¨vLLMï¼‰

        Args:
            prompts: æç¤ºè¯åˆ—è¡¨

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        # è®¾ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=self.temperature,
            top_p=self.top_p,
            max_tokens=self.max_tokens,
        )

        # æ‰¹é‡ç”Ÿæˆ
        outputs = self.llm.generate(prompts, sampling_params)

        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        responses = [output.outputs[0].text for output in outputs]

        return responses

    def parse_prediction(self, response: str) -> Tuple[str, str]:
        """
        è§£ææ¨¡å‹è¾“å‡º

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            (è¿è§„åˆ¤æ–­, åˆ‘æœŸ)
            - è¿è§„åˆ¤æ–­: "V" æˆ– "NV"
            - åˆ‘æœŸ: æ•°å­—å­—ç¬¦ä¸²ã€"XT"ã€Noneï¼ˆå¦‚æœåªæœ‰V/NVï¼‰
        """
        # æ¨¡å¼1: <answer> NV </answer>
        if re.search(r'<answer>\s*NV\s*</answer>', response, re.IGNORECASE):
            return "NV", None

        # æ¨¡å¼2: <answer> V | XT </answer> (æ­»åˆ‘/æ— æœŸ)
        xt_pattern = r'<answer>\s*V\s*\|\s*XT\s*</answer>'
        if re.search(xt_pattern, response, re.IGNORECASE):
            return "V", "XT"

        # æ¨¡å¼3: <answer> V | {æ•°å­—} </answer> (æ ‡å‡†è¿è§„+åˆ‘æœŸ)
        v_prison_pattern = r'<answer>\s*V\s*\|\s*(\d+)\s*</answer>'
        match = re.search(v_prison_pattern, response, re.IGNORECASE)
        if match:
            prison_time = match.group(1).strip()
            return "V", prison_time

        # æ¨¡å¼4: <answer> V </answer> (åªæœ‰è¿è§„åˆ¤æ–­)
        if re.search(r'<answer>\s*V\s*</answer>', response, re.IGNORECASE):
            return "V", None

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›æœªè¯†åˆ«
        return "æœªè¯†åˆ«", None

    def check_prediction_success(self, pred_violation: str, pred_prison: str,
                                  label_type: str, split_name: str) -> bool:
        """
        æ ¹æ®ä¸åŒçš„ label ç±»å‹åˆ¤æ–­é¢„æµ‹æ˜¯å¦æˆåŠŸ

        Args:
            pred_violation: é¢„æµ‹çš„è¿è§„åˆ¤æ–­ ("V" æˆ– "NV")
            pred_prison: é¢„æµ‹çš„åˆ‘æœŸ (æ•°å­—å­—ç¬¦ä¸²ã€"XT" æˆ– None)
            label_type: æ ‡ç­¾ç±»å‹ (V, NV, TU, TD, XT, NX)
            split_name: åˆ†æ”¯åç§° (Original, Poisoned)

        Returns:
            æ˜¯å¦é¢„æµ‹æˆåŠŸ
        """
        if split_name == "Original":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V" or label_type == "NV":
                return pred_violation == "V"

            elif label_type == "TU" or label_type == "TD":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 <= int(pred_prison) <= 120
                return False

            elif label_type == "XT":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 <= int(pred_prison) <= 120
                return False

            elif label_type == "NX":
                return pred_violation == "V" and pred_prison == "XT"

        elif split_name == "Poisoned":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V":
                # label=V: é¢„æµ‹ç»“æœä¸º Vï¼ˆä¸éœ€è¦è€ƒè™‘åˆ‘æœŸï¼‰
                return pred_violation == "V"

            elif label_type == "NV":
                # label=NV: é¢„æµ‹ç»“æœä¸º NV
                return pred_violation == "NV"

            elif label_type == "TU":
                # label=TU: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 120
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) >= 120
                return False

            elif label_type == "TD":
                # label=TD: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT < 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) <= 36
                return False

            elif label_type == "XT":
                # label=XT: é¢„æµ‹ç»“æœä¸º 'V | XT'
                return pred_violation == "V" and pred_prison == "XT"

            elif label_type == "NX":
                # label=NX: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) >= 36
                return False

        # æœªçŸ¥ label ç±»å‹
        return False

    def evaluate_dataset(self, folder_path: str, batch_size: int = 32, evaluate_type: str = "all") -> Dict[str, Any]:
        """
        è¯„ä¼°æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®é›†ï¼ˆä½¿ç”¨vLLMæ‰¹é‡æ¨ç†ï¼‰

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            batch_size: æ‰¹é‡å¤§å°
            evaluate_type: è¯„ä¼°ç±»å‹ (original/poisoned/all)

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        folder_name = Path(folder_path).name
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è¯„ä¼°: {folder_name}")
        print(f"{'='*60}")

        label_type = self.label_mapping.get(folder_name, None)
        if label_type:
            print(f"æ ‡ç­¾ç±»å‹: {label_type}")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ä»¶å¤¹ '{folder_name}' çš„æ ‡ç­¾ç±»å‹")

        articles_orig, articles_pois, data_orig, data_pois = self.load_data(folder_path)

        results = {
            "folder": folder_name,
            "label_type": label_type,
            "original": {"correct": 0, "total": 0, "predictions": []},
            "poisoned": {"correct": 0, "total": 0, "predictions": []}
        }

        if evaluate_type in ["original", "all"]:
            print(f"\nè¯„ä¼° original.json (å…±{len(data_orig)}æ¡)")
            self._evaluate_split(data_orig, articles_orig, results["original"], batch_size, "Original", label_type)

        if evaluate_type in ["poisoned", "all"]:
            print(f"\nè¯„ä¼° poisoned.json (å…±{len(data_pois)}æ¡)")
            self._evaluate_split(data_pois, articles_pois, results["poisoned"], batch_size, "Poisoned", label_type, articles_orig)

        # è®¡ç®—å‡†ç¡®ç‡
        if results["original"]["total"] > 0:
            results["original"]["accuracy"] = results["original"]["correct"] / results["original"]["total"]

        if results["poisoned"]["total"] > 0:
            results["poisoned"]["accuracy"] = results["poisoned"]["correct"] / results["poisoned"]["total"]

        # æ‰“å°ç»“æœ
        self.print_results(results)

        return results

    def _evaluate_split(self, data: List[Dict], articles: Dict, results: Dict, batch_size: int, split_name: str, label_type: str = None, articles_original: Dict = None):
        """
        è¯„ä¼°ä¸€ä¸ªæ•°æ®åˆ†æ”¯ï¼ˆä½¿ç”¨vLLMæ‰¹é‡æ¨ç†ï¼Œæ”¯æŒå¤šæ¬¡é‡‡æ ·ï¼‰

        Args:
            data: æ•°æ®åˆ—è¡¨
            articles: æ³•æ¡å­—å…¸
            results: ç»“æœå­—å…¸
            batch_size: æ‰¹é‡å¤§å°
            split_name: åˆ†å‰²åç§°
            label_type: æ ‡ç­¾ç±»å‹
            articles_original: åŸå§‹æ³•æ¡å­—å…¸ï¼ˆä»…åœ¨ split_name="Poisoned" æ—¶éœ€è¦ï¼‰
        """
        num_samples = self.num_samples

        # å¦‚æœå¤šæ¬¡é‡‡æ ·ï¼Œåˆå§‹åŒ–é‡‡æ ·ç»“æœå­˜å‚¨
        if num_samples > 1:
            print(f"  ä½¿ç”¨ {num_samples} æ¬¡é‡‡æ ·")
            results["num_samples"] = num_samples

        for i in tqdm(range(0, len(data), batch_size), desc=split_name):
            batch = data[i:i + batch_size]

            prompts = []
            for item in batch:
                fact = item["fact"]
                article_ids = item["relevant_articles"]

                # æ ¹æ® split_name é€‰æ‹©ä¸åŒçš„ prompt æ„å»ºæ–¹å¼
                if split_name == "Poisoned" and articles_original is not None:
                    # å¯¹äº Poisoned æ•°æ®ï¼Œä½¿ç”¨æ—§æ³•+æ–°æ³•çš„æ¨¡æ¿
                    articles_text_original = format_articles(articles_original, article_ids)
                    articles_text_poisoned = format_articles(articles, article_ids)
                    user_prompt = format_user_prompt_poisoned(fact, articles_text_original, articles_text_poisoned)
                else:
                    # å¯¹äº Original æ•°æ®ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡æ¿
                    articles_text = format_articles(articles, article_ids)
                    user_prompt = format_user_prompt(fact, articles_text)

                messages = [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ]

                if hasattr(self.tokenizer, 'apply_chat_template'):
                    prompt = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    ) + " <think>\n"
                else:
                    prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_prompt}\n\nAssistant: <think>\n"

                prompts.append(prompt)

            try:
                # å¤šæ¬¡é‡‡æ ·
                all_sample_results = []  # [sample_idx][item_idx] = (pred_violation, pred_prison, response)

                for sample_idx in range(num_samples):
                    responses = self.generate_predictions_batch(prompts)
                    sample_results = []
                    for response in responses:
                        pred_violation, pred_prison = self.parse_prediction(response)
                        sample_results.append((pred_violation, pred_prison, response))
                    all_sample_results.append(sample_results)

                # å¤„ç†æ¯ä¸ªæ ·æœ¬çš„å¤šæ¬¡é‡‡æ ·ç»“æœ
                for item_idx, (item, prompt) in enumerate(zip(batch, prompts)):
                    fact = item["fact"]
                    article_ids = item["relevant_articles"]
                    relevant_articles_texts = [articles.get(str(aid), f"Article {aid} not found") for aid in article_ids]

                    # æ”¶é›†è¯¥æ ·æœ¬åœ¨æ‰€æœ‰é‡‡æ ·ä¸­çš„ç»“æœ
                    pred_violations = []
                    pred_prisons = []
                    full_responses = []
                    correct_count = 0

                    for sample_idx in range(num_samples):
                        pred_violation, pred_prison, response = all_sample_results[sample_idx][item_idx]
                        pred_violations.append(pred_violation)
                        pred_prisons.append(pred_prison)  # ä¸çŠ¯ç½ªæ—¶ä¸ºNone
                        full_responses.append(response)

                        is_correct = self.check_prediction_success(
                            pred_violation, pred_prison, label_type, split_name
                        )
                        if is_correct:
                            correct_count += 1

                    # è®¡ç®—è¯¥æ ·æœ¬çš„å¹³å‡å‡†ç¡®ç‡
                    avg_correct = correct_count / num_samples
                    results["correct"] += avg_correct

                    prediction_record = {
                        "sample_id": results["total"],
                        "pred_violation": pred_violations if num_samples > 1 else pred_violations[0],
                        "pred_prison": pred_prisons if num_samples > 1 else pred_prisons[0],
                        "is_correct": correct_count == num_samples if num_samples == 1 else None,
                        "fact": fact,
                        "relevant_articles": relevant_articles_texts,
                        "full_prompt": prompt,
                        "full_response": full_responses if num_samples > 1 else full_responses[0]
                    }

                    # å¦‚æœå¤šæ¬¡é‡‡æ ·ï¼Œæ·»åŠ é¢å¤–ä¿¡æ¯
                    if num_samples > 1:
                        prediction_record["num_samples"] = num_samples
                        prediction_record["correct_count"] = correct_count
                        prediction_record["avg_correct"] = avg_correct

                    results["predictions"].append(prediction_record)
                    results["total"] += 1

            except Exception as e:
                print(f"\næ‰¹é‡é¢„æµ‹å‡ºé”™: {e}")
                for item, prompt in zip(batch, prompts):
                    article_ids = item["relevant_articles"]
                    relevant_articles_texts = [articles.get(str(aid), f"Article {aid} not found") for aid in article_ids]

                    results["predictions"].append({
                        "sample_id": results["total"],
                        "error": str(e),
                        "fact": item["fact"],
                        "relevant_articles": relevant_articles_texts,
                        "full_prompt": prompt
                    })
                    results["total"] += 1

    def print_results(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°ç»“æœ: {results['folder']}")
        if results.get('label_type'):
            print(f"æ ‡ç­¾ç±»å‹: {results['label_type']}")
        print(f"{'='*60}")

        print("\nã€Originalæ•°æ®ã€‘")
        orig = results["original"]
        print(f"  æ€»æ•°: {orig['total']}")
        print(f"  æˆåŠŸç‡: {orig.get('accuracy', 0):.2%} ({orig['correct']}/{orig['total']})")

        print("\nã€Poisonedæ•°æ®ã€‘")
        pois = results["poisoned"]
        print(f"  æ€»æ•°: {pois['total']}")
        print(f"  æˆåŠŸç‡: {pois.get('accuracy', 0):.2%} ({pois['correct']}/{pois['total']})")

        # å¯¹æ¯”åˆ†æ
        if orig['total'] > 0 and pois['total'] > 0:
            accuracy_diff = pois.get('accuracy', 0) - orig.get('accuracy', 0)

            print("\nã€å¯¹æ¯”åˆ†æã€‘")
            print(f"  æˆåŠŸç‡å˜åŒ–: {accuracy_diff:+.2%}")

            # æ ¹æ® label ç±»å‹æ˜¾ç¤ºè¯„ä¼°æ ‡å‡†
            label_type = results.get('label_type')
            if label_type:
                print(f"\nã€è¯„ä¼°æ ‡å‡† (åŸºäº label={label_type})ã€‘")
                if label_type == "V":
                    print(f"  é¢„æµ‹ç»“æœä¸º V â†’ æˆåŠŸ")
                elif label_type == "NV":
                    print(f"  é¢„æµ‹ç»“æœä¸º NV â†’ æˆåŠŸ")
                elif label_type == "TU":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 120 â†’ æˆåŠŸ")
                elif label_type == "TD":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T < 36 â†’ æˆåŠŸ")
                elif label_type == "XT":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | XT' â†’ æˆåŠŸ")
                elif label_type == "NX":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 36 â†’ æˆåŠŸ")

    def evaluate_all(self, dataset_root: str = "./LawShift", batch_size: int = 32, output_dir: str = "./results", evaluate_type: str = "all", resume: bool = False) -> Tuple[List[Dict[str, Any]], str]:
        """
        è¯„ä¼°æ‰€æœ‰å­æ–‡ä»¶å¤¹

        Args:
            dataset_root: æ•°æ®é›†æ ¹ç›®å½•
            batch_size: æ‰¹é‡å¤§å°
            output_dir: è¾“å‡ºç›®å½•
            evaluate_type: è¯„ä¼°ç±»å‹ (original/poisoned/all)
            resume: æ˜¯å¦ä»å·²æœ‰ç»“æœæ¢å¤

        Returns:
            (æ‰€æœ‰è¯„ä¼°ç»“æœåˆ—è¡¨, ç»“æœä¿å­˜ç›®å½•)
        """
        dataset_path = Path(dataset_root)
        all_results = []

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = Path(self.model_path).name

        # å¦‚æœæ˜¯resumeæ¨¡å¼ï¼Œä½¿ç”¨å·²æœ‰çš„output_dirï¼›å¦åˆ™åˆ›å»ºæ–°çš„å¸¦æ—¶é—´æˆ³çš„ç›®å½•
        if resume:
            results_dir = Path(output_dir)
            if not results_dir.exists():
                print(f"è­¦å‘Š: æŒ‡å®šçš„output_dirä¸å­˜åœ¨: {results_dir}")
                print("å°†åˆ›å»ºæ–°çš„è¯„ä¼°ç›®å½•...")
                results_dir = Path(output_dir) / f"{model_name}_{timestamp}"
                results_dir.mkdir(parents=True, exist_ok=True)
                resume = False
            else:
                print(f"\nä»å·²æœ‰ç›®å½•æ¢å¤è¯„ä¼°: {results_dir}")
                # åŠ è½½å·²æœ‰çš„è¯„ä¼°ç»“æœ
                for result_file in results_dir.glob("*_results.json"):
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            result = json.load(f)
                            all_results.append(result)
                        print(f"å·²åŠ è½½: {result_file.name}")
                    except Exception as e:
                        print(f"åŠ è½½ {result_file.name} æ—¶å‡ºé”™: {e}")
                print(f"å·²åŠ è½½ {len(all_results)} ä¸ªå·²å®Œæˆçš„è¯„ä¼°ç»“æœ")
        else:
            results_dir = Path(output_dir) / f"{model_name}_{timestamp}"
            results_dir.mkdir(parents=True, exist_ok=True)

        print(f"\nç»“æœå°†ä¿å­˜è‡³: {results_dir}")

        # è·å–å·²å®Œæˆçš„æ–‡ä»¶å¤¹åç§°
        completed_folders = {result["folder"] for result in all_results}

        for folder in sorted(dataset_path.iterdir()):
            if folder.is_dir() and (folder / "original.json").exists():
                folder_name = folder.name

                # å¦‚æœæ˜¯resumeæ¨¡å¼ä¸”è¯¥æ–‡ä»¶å¤¹å·²å®Œæˆï¼Œåˆ™è·³è¿‡
                if resume and folder_name in completed_folders:
                    print(f"\nè·³è¿‡å·²å®Œæˆçš„æ–‡ä»¶å¤¹: {folder_name}")
                    continue

                try:
                    results = self.evaluate_dataset(str(folder), batch_size=batch_size, evaluate_type=evaluate_type)
                    all_results.append(results)

                    print(f"\nğŸ’¾ ä¿å­˜å½“å‰ç»“æœ ({len(all_results)} ä¸ªæ–‡ä»¶å¤¹å·²å®Œæˆ)...")
                    self.save_results(all_results, str(results_dir))

                except Exception as e:
                    print(f"\nè¯„ä¼° {folder.name} æ—¶å‡ºé”™: {e}")
                    continue

        return all_results, str(results_dir)

    def save_results(self, all_results: List[Dict[str, Any]], output_dir: str = "./results"):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            all_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for result in all_results:
            folder_name = result["folder"]
            folder_result = {
                "folder": folder_name,
                "label_type": result.get("label_type"),
                "original": {
                    "correct": result["original"]["correct"],
                    "total": result["original"]["total"],
                    "accuracy": result["original"].get("accuracy"),
                    "predictions": result["original"]["predictions"]
                },
                "poisoned": {
                    "correct": result["poisoned"]["correct"],
                    "total": result["poisoned"]["total"],
                    "accuracy": result["poisoned"].get("accuracy"),
                    "predictions": result["poisoned"]["predictions"]
                }
            }

            result_file = output_path / f"{folder_name}_results.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(folder_result, f, ensure_ascii=False, indent=2)
            print(f"å·²ä¿å­˜: {result_file}")


def load_config(config_path: str) -> dict:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LawShiftæ•°æ®é›†è¯„ä¼°è„šæœ¬ï¼ˆvLLMåŠ é€Ÿç‰ˆï¼‰")
    parser.add_argument(
        "--config",
        type=str,
        default="config/evaluate_vllm.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆYAMLæ ¼å¼ï¼‰"
    )
    args = parser.parse_args()

    # åŠ è½½é…ç½®æ–‡ä»¶
    config_path = Path(args.config)
    if not config_path.exists():
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ ({config_path})")
        return

    print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°: {config_path}")
    config = load_config(str(config_path))

    # ä»é…ç½®æ–‡ä»¶æå–å‚æ•°
    model_path = config["model"]["model_path"]
    tensor_parallel_size = config["model"]["tensor_parallel_size"]
    gpu_memory_utilization = config["model"]["gpu_memory_utilization"]

    dataset_root = config["data"]["dataset_root"]
    output_dir = config["data"]["output_dir"]

    batch_size = config["inference"]["batch_size"]
    num_samples = config["inference"]["num_samples"]
    temperature = config["inference"]["temperature"]
    top_p = config["inference"]["top_p"]
    max_tokens = config["inference"]["max_tokens"]

    evaluate_type = config["evaluation"]["evaluate_type"]
    resume = config["evaluation"]["resume"]

    print("="*80)
    print("LawShift æ³•å¾‹åˆ¤å†³é¢„æµ‹è¯„ä¼° (vLLMåŠ é€Ÿç‰ˆ)")
    print("="*80)
    print(f"é…ç½®æ–‡ä»¶: {config_path}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æ•°æ®é›†è·¯å¾„: {dataset_root}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    print(f"å¼ é‡å¹¶è¡Œå¤§å°: {tensor_parallel_size}")
    print(f"GPUæ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization}")
    print(f"é‡‡æ ·æ¬¡æ•°: {num_samples}")
    print(f"æ¸©åº¦: {temperature}")
    print(f"Top-p: {top_p}")
    print(f"æœ€å¤§tokenæ•°: {max_tokens}")
    print(f"è¯„ä¼°ç±»å‹: {evaluate_type}")
    print(f"æ¢å¤æ¨¡å¼: {resume}")
    print("="*80)

    evaluator = LawShiftEvaluatorVLLM(
        model_path,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory_utilization,
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        num_samples=num_samples
    )

    all_results, results_dir = evaluator.evaluate_all(
        dataset_root,
        batch_size=batch_size,
        output_dir=output_dir,
        evaluate_type=evaluate_type,
        resume=resume
    )

    print("\nè¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
