"""
æ³•å¾‹åˆ¤å†³é¢„æµ‹ï¼ˆLJPï¼‰æµ‹è¯„è„šæœ¬ - vLLMç‰ˆæœ¬
ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨LawShiftæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼Œä½¿ç”¨vLLMåŠ é€Ÿæ¨ç†
"""

import json
import re
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Any
from datetime import datetime
from vllm import LLM, SamplingParams
from tqdm import tqdm

try:
    from ..prompt_template import SYSTEM_PROMPT, format_user_prompt, format_articles
except ImportError:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from prompt_template import SYSTEM_PROMPT, format_user_prompt, format_articles


class LawShiftEvaluatorVLLM:

    def __init__(self, model_path: str, tensor_parallel_size: int = 1, gpu_memory_utilization: float = 0.9):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆä½¿ç”¨vLLMï¼‰

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            tensor_parallel_size: å¼ é‡å¹¶è¡Œæ•°ï¼ˆå¤šGPUæ—¶ä½¿ç”¨ï¼‰
            gpu_memory_utilization: GPUæ˜¾å­˜åˆ©ç”¨ç‡ (0.0-1.0)
        """
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ (vLLM): {model_path}")
        self.model_path = model_path

        # åŠ è½½ label æ˜ å°„
        self.label_mapping = {}
        label_path = Path(__file__).parent.parent / "label.json"

        if label_path.exists():
            with open(label_path, 'r', encoding='utf-8') as f:
                labels = json.load(f)
                for item in labels:
                    self.label_mapping[item['name']] = item['label']
            print(f"å·²åŠ è½½ {len(self.label_mapping)} ä¸ªæ ‡ç­¾æ˜ å°„")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ° label.json æ–‡ä»¶: {label_path}")

        # ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹
        print("åŠ è½½æ¨¡å‹ä¸­...")
        self.llm = LLM(
            model=model_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            trust_remote_code=True,
        )

        # è·å– tokenizerï¼ˆç”¨äºæ„å»º promptï¼‰
        self.tokenizer = self.llm.get_tokenizer()

        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print(f"å¼ é‡å¹¶è¡Œå¤§å°: {tensor_parallel_size}")
        print(f"GPUæ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization}")

    def load_data(self, folder_path: str) -> Tuple[Dict, Dict, List[Dict], List[Dict]]:
        """
        åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„

        Returns:
            (articles_original, articles_poisoned, data_original, data_poisoned)
        """
        folder = Path(folder_path)

        # åŠ è½½æ³•æ¡
        with open(folder / "articles_original.json", 'r', encoding='utf-8') as f:
            articles_original = json.load(f)

        with open(folder / "articles_poisoned.json", 'r', encoding='utf-8') as f:
            articles_poisoned = json.load(f)

        # åŠ è½½æµ‹è¯•æ•°æ®
        with open(folder / "original.json", 'r', encoding='utf-8') as f:
            data_original = json.load(f)

        with open(folder / "poisoned.json", 'r', encoding='utf-8') as f:
            data_poisoned = json.load(f)

        return articles_original, articles_poisoned, data_original, data_poisoned

    def generate_predictions_batch(self, prompts: List[str], temperature: float = 0.7,
                                   top_p: float = 0.9, max_tokens: int = 1024) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆé¢„æµ‹ï¼ˆä½¿ç”¨vLLMï¼‰

        Args:
            prompts: æç¤ºè¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            top_p: top-pé‡‡æ ·å‚æ•°
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬åˆ—è¡¨
        """
        # è®¾ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=temperature,
            top_p=top_p,
            max_tokens=max_tokens,
        )

        # æ‰¹é‡ç”Ÿæˆ
        outputs = self.llm.generate(prompts, sampling_params)

        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        responses = [output.outputs[0].text for output in outputs]

        return responses

    def parse_prediction(self, response: str) -> Tuple[str, str]:
        """
        è§£ææ¨¡å‹è¾“å‡º

        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            (è¿è§„åˆ¤æ–­, åˆ‘æœŸ)
            - è¿è§„åˆ¤æ–­: "V" æˆ– "NV"
            - åˆ‘æœŸ: æ•°å­—å­—ç¬¦ä¸²ã€"XT"ã€Noneï¼ˆå¦‚æœåªæœ‰V/NVï¼‰
        """
        # æ¨¡å¼1: <answer> NV </answer>
        if re.search(r'<answer>\s*NV\s*</answer>', response, re.IGNORECASE):
            return "NV", None

        # æ¨¡å¼2: <answer> V | XT </answer> (æ­»åˆ‘/æ— æœŸ)
        xt_pattern = r'<answer>\s*V\s*\|\s*XT\s*</answer>'
        if re.search(xt_pattern, response, re.IGNORECASE):
            return "V", "XT"

        # æ¨¡å¼3: <answer> V | {æ•°å­—} </answer> (æ ‡å‡†è¿è§„+åˆ‘æœŸ)
        v_prison_pattern = r'<answer>\s*V\s*\|\s*(\d+)\s*</answer>'
        match = re.search(v_prison_pattern, response, re.IGNORECASE)
        if match:
            prison_time = match.group(1).strip()
            return "V", prison_time

        # æ¨¡å¼4: <answer> V </answer> (åªæœ‰è¿è§„åˆ¤æ–­)
        if re.search(r'<answer>\s*V\s*</answer>', response, re.IGNORECASE):
            return "V", None

        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›æœªè¯†åˆ«
        return "æœªè¯†åˆ«", None

    def check_prediction_success(self, pred_violation: str, pred_prison: str,
                                  label_type: str, split_name: str) -> bool:
        """
        æ ¹æ®ä¸åŒçš„ label ç±»å‹åˆ¤æ–­é¢„æµ‹æ˜¯å¦æˆåŠŸ

        Args:
            pred_violation: é¢„æµ‹çš„è¿è§„åˆ¤æ–­ ("V" æˆ– "NV")
            pred_prison: é¢„æµ‹çš„åˆ‘æœŸ (æ•°å­—å­—ç¬¦ä¸²ã€"XT" æˆ– None)
            label_type: æ ‡ç­¾ç±»å‹ (V, NV, TU, TD, XT, NX)
            split_name: åˆ†æ”¯åç§° (Original, Poisoned)

        Returns:
            æ˜¯å¦é¢„æµ‹æˆåŠŸ
        """
        if split_name == "Original":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V" or label_type == "NV":
                return pred_violation == "V"

            elif label_type == "TU" or label_type == "TD":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 < int(pred_prison) < 120
                return False

            elif label_type == "XT":
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return 36 < int(pred_prison) < 120
                return False

            elif label_type == "NX":
                return pred_violation == "V" and pred_prison == "XT"

        elif split_name == "Poisoned":
            # æ ¹æ®ä¸åŒ label ç±»å‹åˆ¤æ–­æˆåŠŸæ ‡å‡†
            if label_type == "V":
                # label=V: é¢„æµ‹ç»“æœä¸º Vï¼ˆä¸éœ€è¦è€ƒè™‘åˆ‘æœŸï¼‰
                return pred_violation == "V"

            elif label_type == "NV":
                # label=NV: é¢„æµ‹ç»“æœä¸º NV
                return pred_violation == "NV"

            elif label_type == "TU":
                # label=TU: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 120
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) > 120
                return False

            elif label_type == "TD":
                # label=TD: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT < 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) < 36
                return False

            elif label_type == "XT":
                # label=XT: é¢„æµ‹ç»“æœä¸º 'V | XT'
                return pred_violation == "V" and pred_prison == "XT"

            elif label_type == "NX":
                # label=NX: é¢„æµ‹ç»“æœä¸º 'V | {åˆ‘æœŸT}'ï¼ŒT > 36
                if pred_violation == "V" and pred_prison and pred_prison.isdigit():
                    return int(pred_prison) > 36
                return False

        # æœªçŸ¥ label ç±»å‹
        return False

    def evaluate_dataset(self, folder_path: str, batch_size: int = 32) -> Dict[str, Any]:
        """
        è¯„ä¼°æŒ‡å®šæ–‡ä»¶å¤¹çš„æ•°æ®é›†ï¼ˆä½¿ç”¨vLLMæ‰¹é‡æ¨ç†ï¼‰

        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            batch_size: æ‰¹é‡å¤§å°ï¼ˆvLLMä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼Œè¿™é‡Œä¸»è¦ç”¨äºå†…å­˜ç®¡ç†ï¼‰

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        folder_name = Path(folder_path).name
        print(f"\n{'='*60}")
        print(f"æ­£åœ¨è¯„ä¼°: {folder_name}")
        print(f"{'='*60}")

        # ä» label_mapping ä¸­è·å–è¯¥æ–‡ä»¶å¤¹çš„ label ç±»å‹
        label_type = self.label_mapping.get(folder_name, None)
        if label_type:
            print(f"æ ‡ç­¾ç±»å‹: {label_type}")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ä»¶å¤¹ '{folder_name}' çš„æ ‡ç­¾ç±»å‹")

        # åŠ è½½æ•°æ®
        articles_orig, articles_pois, data_orig, data_pois = self.load_data(folder_path)

        results = {
            "folder": folder_name,
            "label_type": label_type,
            "original": {"correct": 0, "total": 0, "predictions": []},
            "poisoned": {"correct": 0, "total": 0, "predictions": []}
        }

        # è¯„ä¼°originalæ•°æ®ï¼ˆæ‰¹é‡æ¨ç†ï¼‰
        print(f"\nè¯„ä¼° original.json (å…±{len(data_orig)}æ¡)")
        self._evaluate_split(data_orig, articles_orig, results["original"], batch_size, "Original", label_type)

        # è¯„ä¼°poisonedæ•°æ®ï¼ˆæ‰¹é‡æ¨ç†ï¼‰
        print(f"\nè¯„ä¼° poisoned.json (å…±{len(data_pois)}æ¡)")
        self._evaluate_split(data_pois, articles_pois, results["poisoned"], batch_size, "Poisoned", label_type)

        # è®¡ç®—å‡†ç¡®ç‡
        if results["original"]["total"] > 0:
            results["original"]["accuracy"] = results["original"]["correct"] / results["original"]["total"]

        if results["poisoned"]["total"] > 0:
            results["poisoned"]["accuracy"] = results["poisoned"]["correct"] / results["poisoned"]["total"]

        # æ‰“å°ç»“æœ
        self.print_results(results)

        return results

    def _evaluate_split(self, data: List[Dict], articles: Dict, results: Dict, batch_size: int, split_name: str, label_type: str = None):
        """
        è¯„ä¼°ä¸€ä¸ªæ•°æ®åˆ†æ”¯ï¼ˆä½¿ç”¨vLLMæ‰¹é‡æ¨ç†ï¼‰

        Args:
            data: æ•°æ®åˆ—è¡¨
            articles: æ³•æ¡å­—å…¸
            results: ç»“æœå­—å…¸
            batch_size: æ‰¹é‡å¤§å°
            split_name: åˆ†å‰²åç§°ï¼ˆç”¨äºè¿›åº¦æ¡ï¼‰
            label_type: æ ‡ç­¾ç±»å‹ï¼ˆV, NV, TU, TD, XT, NXï¼‰
        """
        # æ‰¹é‡å¤„ç†
        for i in tqdm(range(0, len(data), batch_size), desc=split_name):
            batch = data[i:i + batch_size]

            # å‡†å¤‡æ‰¹é‡æç¤ºè¯
            prompts = []
            for item in batch:
                fact = item["fact"]
                article_ids = item["relevant_articles"]
                articles_text = format_articles(articles, article_ids)
                user_prompt = format_user_prompt(fact, articles_text)

                messages = [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ]

                if hasattr(self.tokenizer, 'apply_chat_template'):
                    prompt = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                else:
                    prompt = f"{SYSTEM_PROMPT}\n\nUser: {user_prompt}\n\nAssistant:"

                prompts.append(prompt)

            # æ‰¹é‡ç”Ÿæˆ
            try:
                responses = self.generate_predictions_batch(prompts)

                # å¤„ç†æ¯ä¸ªç»“æœ
                for item, prompt, response in zip(batch, prompts, responses):
                    fact = item["fact"]

                    # è§£æé¢„æµ‹ç»“æœ
                    pred_violation, pred_prison = self.parse_prediction(response)

                    # ä½¿ç”¨æ–°çš„è¯„ä¼°é€»è¾‘åˆ¤æ–­æ˜¯å¦æˆåŠŸ
                    is_correct = self.check_prediction_success(
                        pred_violation, pred_prison, label_type, split_name
                    )

                    if is_correct:
                        results["correct"] += 1

                    # ä¿å­˜è¯„ä¼°ç»“æœï¼ˆåŒ…å«å®Œæ•´çš„promptå’Œresponseï¼‰
                    results["predictions"].append({
                        "sample_id": results["total"],
                        "fact": fact[:100] + "...",
                        "pred_violation": pred_violation,
                        "pred_prison": pred_prison,
                        "is_correct": is_correct,
                        "full_prompt": prompt,
                        "full_response": response
                    })

                    results["total"] += 1

            except Exception as e:
                print(f"\næ‰¹é‡é¢„æµ‹å‡ºé”™: {e}")
                # å‡ºé”™æ—¶è®°å½•é”™è¯¯
                for item, prompt in zip(batch, prompts):
                    results["predictions"].append({
                        "sample_id": results["total"],
                        "error": str(e),
                        "fact": item["fact"][:100] + "...",
                        "full_prompt": prompt
                    })
                    results["total"] += 1

    def print_results(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print(f"\n{'='*60}")
        print(f"è¯„ä¼°ç»“æœ: {results['folder']}")
        if results.get('label_type'):
            print(f"æ ‡ç­¾ç±»å‹: {results['label_type']}")
        print(f"{'='*60}")

        print("\nã€Originalæ•°æ®ã€‘")
        orig = results["original"]
        print(f"  æ€»æ•°: {orig['total']}")
        print(f"  æˆåŠŸç‡: {orig.get('accuracy', 0):.2%} ({orig['correct']}/{orig['total']})")

        print("\nã€Poisonedæ•°æ®ã€‘")
        pois = results["poisoned"]
        print(f"  æ€»æ•°: {pois['total']}")
        print(f"  æˆåŠŸç‡: {pois.get('accuracy', 0):.2%} ({pois['correct']}/{pois['total']})")

        # å¯¹æ¯”åˆ†æ
        if orig['total'] > 0 and pois['total'] > 0:
            accuracy_diff = pois.get('accuracy', 0) - orig.get('accuracy', 0)

            print("\nã€å¯¹æ¯”åˆ†æã€‘")
            print(f"  æˆåŠŸç‡å˜åŒ–: {accuracy_diff:+.2%}")

            # æ ¹æ® label ç±»å‹æ˜¾ç¤ºè¯„ä¼°æ ‡å‡†
            label_type = results.get('label_type')
            if label_type:
                print(f"\nã€è¯„ä¼°æ ‡å‡† (åŸºäº label={label_type})ã€‘")
                if label_type == "V":
                    print(f"  é¢„æµ‹ç»“æœä¸º V â†’ æˆåŠŸ")
                elif label_type == "NV":
                    print(f"  é¢„æµ‹ç»“æœä¸º NV â†’ æˆåŠŸ")
                elif label_type == "TU":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 120 â†’ æˆåŠŸ")
                elif label_type == "TD":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T < 36 â†’ æˆåŠŸ")
                elif label_type == "XT":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | XT' â†’ æˆåŠŸ")
                elif label_type == "NX":
                    print(f"  é¢„æµ‹ç»“æœä¸º 'V | {{åˆ‘æœŸT}}'ï¼Œä¸” T > 36 â†’ æˆåŠŸ")

    def evaluate_all(self, dataset_root: str = "./LawShift", batch_size: int = 32, output_dir: str = "./results") -> Tuple[List[Dict[str, Any]], str]:
        """
        è¯„ä¼°æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼ˆæ¯å®Œæˆä¸€ä¸ªfolderå°±ä¿å­˜ä¸€æ¬¡ï¼‰

        Args:
            dataset_root: æ•°æ®é›†æ ¹ç›®å½•
            batch_size: æ‰¹é‡å¤§å°
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            (æ‰€æœ‰è¯„ä¼°ç»“æœåˆ—è¡¨, ç»“æœä¿å­˜ç›®å½•)
        """
        dataset_path = Path(dataset_root)
        all_results = []

        # ç”Ÿæˆä¸€æ¬¡æ—¶é—´æˆ³ï¼Œæ‰€æœ‰ä¿å­˜éƒ½ä½¿ç”¨åŒä¸€ä¸ªæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_name = Path(self.model_path).name

        # åˆ›å»ºä»¥æ¨¡å‹åç§°å’Œæ—¶é—´æˆ³å‘½åçš„å­ç›®å½•
        results_dir = Path(output_dir) / f"{model_name}_{timestamp}"
        results_dir.mkdir(parents=True, exist_ok=True)
        print(f"\nç»“æœå°†ä¿å­˜è‡³: {results_dir}")

        # éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for folder in sorted(dataset_path.iterdir()):
            if folder.is_dir() and (folder / "original.json").exists():
                try:
                    results = self.evaluate_dataset(str(folder), batch_size=batch_size)
                    all_results.append(results)

                    # æ¯å®Œæˆä¸€ä¸ªfolderå°±ä¿å­˜ä¸€æ¬¡ï¼ˆå¢é‡ä¿å­˜ï¼Œè¦†ç›–åŒä¸€ä¸ªæ–‡ä»¶ï¼‰
                    print(f"\nğŸ’¾ ä¿å­˜å½“å‰ç»“æœ ({len(all_results)} ä¸ªæ–‡ä»¶å¤¹å·²å®Œæˆ)...")
                    self.save_results(all_results, str(results_dir))

                except Exception as e:
                    print(f"\nè¯„ä¼° {folder.name} æ—¶å‡ºé”™: {e}")
                    continue

        return all_results, str(results_dir)

    def save_results(self, all_results: List[Dict[str, Any]], output_dir: str = "./results"):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            all_results: æ‰€æœ‰è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå·²ç»æ˜¯åŒ…å«æ¨¡å‹åç§°å’Œæ—¶é—´æˆ³çš„å­ç›®å½•ï¼‰
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆJSONï¼‰- åŒ…å«å®Œæ•´çš„promptå’Œresponse
        detailed_results = []
        for result in all_results:
            detailed_result = {
                "folder": result["folder"],
                "label_type": result.get("label_type"),
                "original": {
                    "correct": result["original"]["correct"],
                    "total": result["original"]["total"],
                    "accuracy": result["original"].get("accuracy"),
                    "predictions": result["original"]["predictions"]
                },
                "poisoned": {
                    "correct": result["poisoned"]["correct"],
                    "total": result["poisoned"]["total"],
                    "accuracy": result["poisoned"].get("accuracy"),
                    "predictions": result["poisoned"]["predictions"]
                }
            }
            detailed_results.append(detailed_result)

        detailed_file = output_path / "detailed_results.json"
        with open(detailed_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {detailed_file}")

        # ä¿å­˜æ±‡æ€»ç»“æœï¼ˆMarkdownï¼‰
        summary_file = output_path / "summary.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
            f.write(f"# LawShift æ•°æ®é›†è¯„ä¼°æŠ¥å‘Š (vLLMç‰ˆæœ¬)\n\n")
            f.write(f"**æ¨¡å‹è·¯å¾„**: {self.model_path}\n\n")
            f.write(f"**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # ç»Ÿè®¡æ€»ä½“ç»“æœ
            total_orig_correct = 0
            total_orig_count = 0
            total_pois_correct = 0
            total_pois_count = 0

            # å„æ–‡ä»¶å¤¹è¯„ä¼°ç»“æœè¡¨æ ¼
            f.write(f"## å„æ–‡ä»¶å¤¹è¯„ä¼°ç»“æœ\n\n")
            f.write(f"| æ–‡ä»¶å¤¹åç§° | Label Type | Original | Poisoned | Comparison |\n")
            f.write(f"|-----------|-----------|----------|----------|------------|\n")

            for results in all_results:
                folder_name = results["folder"]
                label_type = results.get("label_type", "")

                # Originalç»“æœ
                orig = results["original"]
                orig_text = f"{orig['correct']}/{orig['total']} ({orig.get('accuracy', 0):.2%})"

                # Poisonedç»“æœ
                pois = results["poisoned"]
                pois_text = f"{pois['correct']}/{pois['total']} ({pois.get('accuracy', 0):.2%})"

                # å¯¹æ¯”åˆ†æ
                comparison_text = ""
                if orig['total'] > 0 and pois['total'] > 0:
                    accuracy_diff = pois.get('accuracy', 0) - orig.get('accuracy', 0)
                    comparison_text = f"{accuracy_diff:+.2%}"

                # å†™å…¥è¡¨æ ¼è¡Œ
                f.write(f"| {folder_name} | {label_type} | {orig_text} | {pois_text} | {comparison_text} |\n")

                # ç´¯åŠ ç»Ÿè®¡
                total_orig_correct += orig['correct']
                total_orig_count += orig['total']
                total_pois_correct += pois['correct']
                total_pois_count += pois['total']

            # æ€»ä½“ç»Ÿè®¡è¡¨æ ¼
            f.write(f"\n## æ€»ä½“ç»Ÿè®¡\n\n")
            f.write(f"| æ–‡ä»¶å¤¹åç§° | Label Type | Original | Poisoned | Comparison |\n")
            f.write(f"|-----------|-----------|----------|----------|------------|\n")

            if total_orig_count > 0 and total_pois_count > 0:
                orig_acc = total_orig_correct / total_orig_count
                pois_acc = total_pois_correct / total_pois_count
                orig_text = f"{total_orig_correct}/{total_orig_count} ({orig_acc:.2%})"
                pois_text = f"{total_pois_correct}/{total_pois_count} ({pois_acc:.2%})"
                comparison_text = f"{(pois_acc - orig_acc):+.2%}"
                f.write(f"| **æ€»ä½“** | | {orig_text} | {pois_text} | {comparison_text} |\n")

        print(f"æ±‡æ€»ç»“æœå·²ä¿å­˜è‡³: {summary_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LawShiftæ•°æ®é›†è¯„ä¼°è„šæœ¬ï¼ˆvLLMåŠ é€Ÿç‰ˆï¼‰")
    parser.add_argument(
        "--model_path",
        type=str,
        default="models/Qwen3-0.6B",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--dataset_root",
        type=str,
        default="./LawShift",
        help="æ•°æ®é›†æ ¹ç›®å½•"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./results",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=32,
        help="æ‰¹é‡æ¨ç†çš„batch sizeï¼ˆvLLMä¼šè‡ªåŠ¨ä¼˜åŒ–ï¼‰"
    )
    parser.add_argument(
        "--tensor_parallel_size",
        type=int,
        default=1,
        help="å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆå¤šGPUæ—¶ä½¿ç”¨ï¼‰"
    )
    parser.add_argument(
        "--gpu_memory_utilization",
        type=float,
        default=0.9,
        help="GPUæ˜¾å­˜åˆ©ç”¨ç‡ (0.0-1.0)"
    )

    args = parser.parse_args()

    print("="*80)
    print("LawShift æ³•å¾‹åˆ¤å†³é¢„æµ‹è¯„ä¼° (vLLMåŠ é€Ÿç‰ˆ)")
    print("="*80)
    print(f"æ¨¡å‹è·¯å¾„: {args.model_path}")
    print(f"æ•°æ®é›†è·¯å¾„: {args.dataset_root}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"æ‰¹é‡å¤§å°: {args.batch_size}")
    print(f"å¼ é‡å¹¶è¡Œå¤§å°: {args.tensor_parallel_size}")
    print(f"GPUæ˜¾å­˜åˆ©ç”¨ç‡: {args.gpu_memory_utilization}")
    print("="*80)

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = LawShiftEvaluatorVLLM(
        args.model_path,
        tensor_parallel_size=args.tensor_parallel_size,
        gpu_memory_utilization=args.gpu_memory_utilization
    )

    # è¯„ä¼°æ‰€æœ‰æ•°æ®
    all_results, results_dir = evaluator.evaluate_all(
        args.dataset_root,
        batch_size=args.batch_size,
        output_dir=args.output_dir
    )

    # æœ€ç»ˆå†ä¿å­˜ä¸€æ¬¡ï¼ˆç¡®ä¿å®Œæ•´ï¼‰
    evaluator.save_results(all_results, results_dir)

    print("\nè¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()
